{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smit-patel-m/SummerInternship/blob/main/PUF_GBNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "EnMG6Vm6nOS1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "from sklearn.base import BaseEstimator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjR786MCs-cw",
        "outputId": "d8c74b9c-80b8-48f5-b67a-f49305975da7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WC86YMmQjgNN",
        "outputId": "a829202e-b2d9-4698-8d74-1dfcb785e547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder '/content/db1' already exists. Skipping unzip.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "if not os.path.exists(\"/content/db1\"):\n",
        "    drive.mount('/content/drive')\n",
        "    !unzip \"/content/drive/MyDrive/Summer Internship/ML Attack.zip\" -d \"/content/db1\"\n",
        "else:\n",
        "    print(\"Folder '/content/db1' already exists. Skipping unzip.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiB-uebsntWz"
      },
      "source": [
        "# Load the Excel files into pandas DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "-rGIVM7Wl0v7"
      },
      "outputs": [],
      "source": [
        "# Load the Excel files into pandas DataFrames\n",
        "\n",
        "response_delay_path = \"/content/db1/ML Attack/Responce_Delay_2us.xlsx\"\n",
        "challenge_path = \"/content/db1/ML Attack/Challenge.xlsx\"\n",
        "response_delay_sheet_names = pd.ExcelFile(response_delay_path).sheet_names\n",
        "response_delay_df = pd.read_excel(response_delay_path, sheet_name=0,header=None)\n",
        "\n",
        "challenge_df = pd.read_excel(challenge_path,header=None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "_mTGh5vBn5Mc",
        "outputId": "79b326ac-62fe-44cb-e884-60a3ce07f5ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Sheet Name: ['FD_MODE_1', 'VD_MODE_1', 'FD_MODE_2', 'VD_MODE_2', 'FD_MODE_3', 'VD_MODE_3', 'FD_MODE_4', 'VD_MODE_4', 'FD_MODE_5', 'VD_MODE_5', 'FD_MODE_6', 'VD_MODE_6', 'FD_MODE_7', 'VD_MODE_7', 'FD_MODE_8', 'VD_MODE_8']\n",
            "Response Delay DataFrame:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   0   1   2   3   4   5   6   7   8   9   ...  22  23  24  25  26  27  28  \\\n",
              "0   0   1   1   1   1   0   1   1   1   1  ...   1   1   1   0   0   0   0   \n",
              "1   1   0   1   1   1   0   0   0   0   1  ...   0   1   1   0   0   0   1   \n",
              "2   1   1   0   0   1   1   0   0   0   0  ...   1   1   0   1   1   0   1   \n",
              "3   1   1   1   0   1   1   0   0   1   1  ...   1   1   0   0   0   1   1   \n",
              "4   0   1   0   0   0   1   0   1   1   1  ...   1   0   1   1   0   0   0   \n",
              "\n",
              "   29  30  31  \n",
              "0   0   0   0  \n",
              "1   0   0   1  \n",
              "2   1   1   1  \n",
              "3   0   1   1  \n",
              "4   1   0   0  \n",
              "\n",
              "[5 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-506a8ea0-ad6b-4b27-81f6-630e4449b3bf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 32 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-506a8ea0-ad6b-4b27-81f6-630e4449b3bf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-506a8ea0-ad6b-4b27-81f6-630e4449b3bf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-506a8ea0-ad6b-4b27-81f6-630e4449b3bf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d018699f-69b9-4835-80f8-93758348c53d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d018699f-69b9-4835-80f8-93758348c53d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d018699f-69b9-4835-80f8-93758348c53d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Challenge DataFrame:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   0   1   2   3   4   5   6   7   8   9   ...  22  23  24  25  26  27  28  \\\n",
              "0   0   1   0   0   0   0   0   1   1   1  ...   1   0   1   1   0   0   0   \n",
              "1   0   1   1   0   0   1   0   1   1   1  ...   0   1   1   0   1   0   0   \n",
              "2   0   0   1   0   1   0   0   1   0   0  ...   0   0   1   1   0   1   1   \n",
              "3   0   0   0   1   1   0   0   0   1   0  ...   1   1   0   0   1   0   0   \n",
              "4   1   1   1   0   1   0   0   0   0   1  ...   1   0   0   0   0   1   0   \n",
              "\n",
              "   29  30  31  \n",
              "0   1   1   1  \n",
              "1   1   0   0  \n",
              "2   0   1   1  \n",
              "3   0   1   1  \n",
              "4   0   1   0  \n",
              "\n",
              "[5 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-65f355e5-6709-4b1f-833c-98dbd67d19de\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 32 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-65f355e5-6709-4b1f-833c-98dbd67d19de')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-65f355e5-6709-4b1f-833c-98dbd67d19de button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-65f355e5-6709-4b1f-833c-98dbd67d19de');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7a461cbc-eb03-47f4-a04e-f5682358ed54\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7a461cbc-eb03-47f4-a04e-f5682358ed54')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7a461cbc-eb03-47f4-a04e-f5682358ed54 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Display the first few rows of each DataFrame to verify\n",
        "print(\"Selected Sheet Name:\", response_delay_sheet_names)\n",
        "\n",
        "print(\"Response Delay DataFrame:\")\n",
        "display(response_delay_df.head())\n",
        "\n",
        "print(\"\\nChallenge DataFrame:\")\n",
        "display(challenge_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "Kp63WbUqpWoC"
      },
      "outputs": [],
      "source": [
        "# Convert to NumPy\n",
        "X = challenge_df.values.astype(np.float32)     # shape: (N, 32)\n",
        "Y = response_delay_df.values.astype(np.float32)  # shape: (N, 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--4ZpOkUpWrT",
        "outputId": "ab317b55-99ac-4af1-c256-44478af04f52"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., ..., 1., 1., 1.],\n",
              "       [0., 1., 1., ..., 1., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 1., 1.],\n",
              "       ...,\n",
              "       [0., 1., 1., ..., 1., 1., 1.],\n",
              "       [1., 1., 0., ..., 0., 0., 1.],\n",
              "       [1., 0., 0., ..., 1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEeFqOM_pWt4",
        "outputId": "fa864680-38c6-44e3-bfcb-f3797cc5f0d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 1., ..., 0., 0., 0.],\n",
              "       [1., 0., 1., ..., 0., 0., 1.],\n",
              "       [1., 1., 0., ..., 1., 1., 1.],\n",
              "       ...,\n",
              "       [0., 1., 0., ..., 1., 1., 0.],\n",
              "       [1., 0., 0., ..., 0., 1., 0.],\n",
              "       [1., 1., 0., ..., 1., 1., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ],
      "source": [
        "Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "er3a7-LsvojT"
      },
      "outputs": [],
      "source": [
        "#X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "\n",
        "\n",
        "def get_next_model_version(base_path=\"/content/drive/MyDrive/SummerIntern/Puf Models\", prefix=\"puf_gbnn\"):\n",
        "    os.makedirs(base_path, exist_ok=True)\n",
        "    existing = [f for f in os.listdir(base_path) if f.startswith(prefix)]\n",
        "    versions = [int(re.findall(r'V(\\d+)', name)[0]) for name in existing if re.search(r'V\\d+', name)]\n",
        "    next_version = max(versions) + 1 if versions else 1\n",
        "    model_folder = f\"{prefix}_V{next_version}\"\n",
        "    return os.path.join(base_path, model_folder)\n",
        "\n",
        "def save_gbnn_model(model, path, history=None):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    model_path = os.path.join(path, \"model.pt\")\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"✅ GBNN model saved at: {model_path}\")\n",
        "\n",
        "    if history is not None:\n",
        "        history_df = pd.DataFrame(history, columns=[\n",
        "            \"Stage\", \"Epoch\", \"Train Accuracy\", \"Val Accuracy\"\n",
        "        ])\n",
        "        excel_path = os.path.join(path, \"training_history.xlsx\")\n",
        "        history_df.to_excel(excel_path, index=False)\n",
        "        print(f\" Training history saved at: {excel_path}\")\n",
        "\n",
        "def load_gbnn_model(model_class, model_args, path):\n",
        "    model_path = os.path.join(path, \"model.pt\")\n",
        "    model = model_class(**model_args)\n",
        "    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
        "    print(f\"✅ GBNN model loaded from: {model_path}\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "1mc-MiMxqp6f"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "fIfZh0Z-7_qx"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# MLP for each stage\n",
        "class MultiOutputNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiOutputNN, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(32, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "def bitwise_accuracy(preds, labels):\n",
        "    return (preds == labels).float().mean().item()\n",
        "\n",
        "def exact_match(preds, labels):\n",
        "    return ((preds == labels).all(dim=1)).float().mean().item()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GBNN(nn.Module):\n",
        "    def __init__(self, n_stages=30):\n",
        "        super(GBNN, self).__init__()\n",
        "        self.n_stages = n_stages\n",
        "        self.models = nn.ModuleList([MultiOutputNN() for _ in range(n_stages)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.zeros(x.size(0), 32).to(x.device)\n",
        "        for model in self.models:\n",
        "            out += model(x)\n",
        "        return out\n",
        "\n",
        "    def stagewise_forward(self, x, up_to_stage):\n",
        "        out = torch.zeros(x.size(0), 32).to(x.device)\n",
        "        for model in self.models[:up_to_stage]:\n",
        "            out += model(x)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "3QFJpCDCs_ru"
      },
      "outputs": [],
      "source": [
        "\n",
        "def split_dataset(dataset):\n",
        "    total_size = len(dataset)\n",
        "    train_size = int(0.7 * total_size)\n",
        "    test_size = int(0.2 * total_size)\n",
        "    val_size = total_size - train_size - test_size\n",
        "    return random_split(dataset, [train_size, test_size, val_size])\n",
        "\n",
        "\n",
        "def train_gbnn(model, X_train, y_train, X_val, y_val, X_test, y_test, n_epochs=200, lr=0.01, save_path=None):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "    X_val, y_val = X_val.to(device), y_val.to(device)\n",
        "    X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "\n",
        "    criterion = nn.MSELoss()  # Or use nn.BCEWithLogitsLoss()\n",
        "\n",
        "    history = []\n",
        "\n",
        "    for stage_idx, sub_model in enumerate(model.models):\n",
        "        print(f\"\\n🎯 Stage {stage_idx + 1}/{model.n_stages}\")\n",
        "        optimizer = optim.Adam(sub_model.parameters(), lr=lr)\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            model.train()\n",
        "            with torch.no_grad():\n",
        "                logits_so_far = model.stagewise_forward(X_train, stage_idx)\n",
        "                pred_prob = torch.sigmoid(logits_so_far)\n",
        "                residual = y_train - pred_prob\n",
        "\n",
        "            pred_logits = sub_model(X_train)\n",
        "            loss = criterion(pred_logits, residual)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Evaluate train/val accuracy & log loss EVERY epoch\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                train_logits = model.forward(X_train)\n",
        "                train_preds = (torch.sigmoid(train_logits) > 0.5).float()\n",
        "                train_acc = bitwise_accuracy(train_preds, y_train)\n",
        "\n",
        "                val_logits = model.forward(X_val)\n",
        "                val_preds = (torch.sigmoid(val_logits) > 0.5).float()\n",
        "                val_acc = bitwise_accuracy(val_preds, y_val)\n",
        "\n",
        "\n",
        "            history.append([\n",
        "                stage_idx + 1,\n",
        "                epoch + 1,\n",
        "                float(train_acc),\n",
        "                float(val_acc),\n",
        "                float(loss.item())\n",
        "            ])\n",
        "\n",
        "\n",
        "            if (epoch % 100 == 0) or (epoch == n_epochs - 1):\n",
        "                print(f\"Epoch {epoch+1}/{n_epochs} | Train Loss: {loss.item():.4f} | \"\n",
        "                      f\"Train Acc: {train_acc*100:.2f}% | Val Acc: {val_acc*100:.2f}%\")\n",
        "\n",
        "\n",
        "    if save_path is not None:\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        # Save model\n",
        "        torch.save(model.state_dict(), os.path.join(save_path, \"model.pt\"))\n",
        "        print(f\"✅ GBNN model saved at: {os.path.join(save_path, 'model.pt')}\")\n",
        "        # Save Excel history\n",
        "        import pandas as pd\n",
        "        history_df = pd.DataFrame(history, columns=[\n",
        "            \"Stage\", \"Epoch\", \"Train Accuracy\", \"Val Accuracy\", \"Train Error\"\n",
        "        ])\n",
        "        history_df.to_excel(os.path.join(save_path, \"training_history.xlsx\"), index=False)\n",
        "        print(f\" Training history saved at: {os.path.join(save_path, 'training_history.xlsx')}\")\n",
        "\n",
        "    # Print final test performance\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_logits = model.forward(X_test)\n",
        "        test_preds = (torch.sigmoid(test_logits) > 0.5).float()\n",
        "        test_acc = bitwise_accuracy(test_preds, y_test)\n",
        "        test_exact = exact_match(test_preds, y_test)\n",
        "    print(f\"\\n✅ Test Acc: {test_acc*100:.2f}% | Test Exact Match: {test_exact*100:.2f}%\")\n",
        "\n",
        "    return model, history\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "save_path = get_next_model_version(\n",
        "    base_path=\"/content/drive/MyDrive/SummerIntern/Puf Models\",\n",
        "    prefix=\"puf_gbnn\"\n",
        ")\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
        "\n",
        "dataset = TensorDataset(X_tensor, Y_tensor)\n",
        "train_set, test_set, val_set = split_dataset(dataset)\n",
        "\n",
        "train_data = next(iter(DataLoader(train_set, batch_size=len(train_set))))\n",
        "test_data = next(iter(DataLoader(test_set, batch_size=len(test_set))))\n",
        "val_data = next(iter(DataLoader(val_set, batch_size=len(val_set))))\n",
        "\n",
        "# Unpack data\n",
        "X_train, y_train = train_data\n",
        "X_val, y_val = val_data\n",
        "X_test, y_test = test_data\n",
        "# Train and save\n",
        "model = GBNN(n_stages=35)\n",
        "trained_model, history = train_gbnn(model, X_train, y_train, X_val, y_val, X_test, y_test, n_epochs=500, lr=0.01, save_path=save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg9XHHhz8Ctt",
        "outputId": "d1e156ca-fe72-4ad2-b0de-4374c712545c"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🎯 Stage 1/35\n",
            "Epoch 1/500 | Train Loss: 0.2584 | Train Acc: 50.64% | Val Acc: 50.61%\n",
            "Epoch 101/500 | Train Loss: 0.2302 | Train Acc: 52.87% | Val Acc: 50.68%\n",
            "Epoch 201/500 | Train Loss: 0.2235 | Train Acc: 53.60% | Val Acc: 50.64%\n",
            "Epoch 301/500 | Train Loss: 0.2209 | Train Acc: 53.94% | Val Acc: 50.74%\n",
            "Epoch 401/500 | Train Loss: 0.2191 | Train Acc: 54.16% | Val Acc: 50.80%\n",
            "Epoch 500/500 | Train Loss: 0.2176 | Train Acc: 54.42% | Val Acc: 50.98%\n",
            "\n",
            "🎯 Stage 2/35\n",
            "Epoch 1/500 | Train Loss: 0.2506 | Train Acc: 54.30% | Val Acc: 51.14%\n",
            "Epoch 101/500 | Train Loss: 0.2234 | Train Acc: 55.43% | Val Acc: 51.24%\n",
            "Epoch 201/500 | Train Loss: 0.2178 | Train Acc: 56.21% | Val Acc: 51.24%\n",
            "Epoch 301/500 | Train Loss: 0.2156 | Train Acc: 56.43% | Val Acc: 51.32%\n",
            "Epoch 401/500 | Train Loss: 0.2138 | Train Acc: 56.67% | Val Acc: 51.00%\n",
            "Epoch 500/500 | Train Loss: 0.2131 | Train Acc: 56.76% | Val Acc: 50.96%\n",
            "\n",
            "🎯 Stage 3/35\n",
            "Epoch 1/500 | Train Loss: 0.2330 | Train Acc: 56.85% | Val Acc: 51.14%\n",
            "Epoch 101/500 | Train Loss: 0.2123 | Train Acc: 58.53% | Val Acc: 50.82%\n",
            "Epoch 201/500 | Train Loss: 0.2079 | Train Acc: 59.04% | Val Acc: 50.82%\n",
            "Epoch 301/500 | Train Loss: 0.2060 | Train Acc: 59.26% | Val Acc: 50.83%\n",
            "Epoch 401/500 | Train Loss: 0.2051 | Train Acc: 59.30% | Val Acc: 50.96%\n",
            "Epoch 500/500 | Train Loss: 0.2047 | Train Acc: 59.30% | Val Acc: 50.79%\n",
            "\n",
            "🎯 Stage 4/35\n",
            "Epoch 1/500 | Train Loss: 0.2276 | Train Acc: 59.26% | Val Acc: 50.85%\n",
            "Epoch 101/500 | Train Loss: 0.2066 | Train Acc: 60.77% | Val Acc: 51.28%\n",
            "Epoch 201/500 | Train Loss: 0.2010 | Train Acc: 61.40% | Val Acc: 51.07%\n",
            "Epoch 301/500 | Train Loss: 0.1987 | Train Acc: 61.68% | Val Acc: 51.01%\n",
            "Epoch 401/500 | Train Loss: 0.1974 | Train Acc: 61.83% | Val Acc: 51.06%\n",
            "Epoch 500/500 | Train Loss: 0.1965 | Train Acc: 61.96% | Val Acc: 51.31%\n",
            "\n",
            "🎯 Stage 5/35\n",
            "Epoch 1/500 | Train Loss: 0.2198 | Train Acc: 61.72% | Val Acc: 51.32%\n",
            "Epoch 101/500 | Train Loss: 0.1967 | Train Acc: 62.78% | Val Acc: 51.29%\n",
            "Epoch 201/500 | Train Loss: 0.1894 | Train Acc: 63.53% | Val Acc: 51.09%\n",
            "Epoch 301/500 | Train Loss: 0.1868 | Train Acc: 63.76% | Val Acc: 51.23%\n",
            "Epoch 401/500 | Train Loss: 0.1855 | Train Acc: 63.89% | Val Acc: 51.07%\n",
            "Epoch 500/500 | Train Loss: 0.1846 | Train Acc: 63.99% | Val Acc: 51.20%\n",
            "\n",
            "🎯 Stage 6/35\n",
            "Epoch 1/500 | Train Loss: 0.2127 | Train Acc: 64.33% | Val Acc: 51.15%\n",
            "Epoch 101/500 | Train Loss: 0.1935 | Train Acc: 64.75% | Val Acc: 50.82%\n",
            "Epoch 201/500 | Train Loss: 0.1895 | Train Acc: 65.18% | Val Acc: 50.96%\n",
            "Epoch 301/500 | Train Loss: 0.1876 | Train Acc: 65.41% | Val Acc: 50.97%\n",
            "Epoch 401/500 | Train Loss: 0.1865 | Train Acc: 65.50% | Val Acc: 50.84%\n",
            "Epoch 500/500 | Train Loss: 0.1858 | Train Acc: 65.56% | Val Acc: 50.73%\n",
            "\n",
            "🎯 Stage 7/35\n",
            "Epoch 1/500 | Train Loss: 0.2055 | Train Acc: 65.99% | Val Acc: 50.83%\n",
            "Epoch 101/500 | Train Loss: 0.1849 | Train Acc: 66.73% | Val Acc: 51.28%\n",
            "Epoch 201/500 | Train Loss: 0.1799 | Train Acc: 67.27% | Val Acc: 51.19%\n",
            "Epoch 301/500 | Train Loss: 0.1780 | Train Acc: 67.47% | Val Acc: 51.21%\n",
            "Epoch 401/500 | Train Loss: 0.1770 | Train Acc: 67.54% | Val Acc: 51.31%\n",
            "Epoch 500/500 | Train Loss: 0.1766 | Train Acc: 67.66% | Val Acc: 51.20%\n",
            "\n",
            "🎯 Stage 8/35\n",
            "Epoch 1/500 | Train Loss: 0.1951 | Train Acc: 67.82% | Val Acc: 51.43%\n",
            "Epoch 101/500 | Train Loss: 0.1770 | Train Acc: 68.62% | Val Acc: 51.28%\n",
            "Epoch 201/500 | Train Loss: 0.1711 | Train Acc: 69.20% | Val Acc: 51.34%\n",
            "Epoch 301/500 | Train Loss: 0.1691 | Train Acc: 69.41% | Val Acc: 51.57%\n",
            "Epoch 401/500 | Train Loss: 0.1680 | Train Acc: 69.59% | Val Acc: 51.59%\n",
            "Epoch 500/500 | Train Loss: 0.1675 | Train Acc: 69.64% | Val Acc: 51.63%\n",
            "\n",
            "🎯 Stage 9/35\n",
            "Epoch 1/500 | Train Loss: 0.1883 | Train Acc: 70.33% | Val Acc: 51.82%\n",
            "Epoch 101/500 | Train Loss: 0.1699 | Train Acc: 71.01% | Val Acc: 51.69%\n",
            "Epoch 201/500 | Train Loss: 0.1643 | Train Acc: 71.59% | Val Acc: 51.83%\n",
            "Epoch 301/500 | Train Loss: 0.1622 | Train Acc: 71.84% | Val Acc: 51.79%\n",
            "Epoch 401/500 | Train Loss: 0.1612 | Train Acc: 71.93% | Val Acc: 51.82%\n",
            "Epoch 500/500 | Train Loss: 0.1607 | Train Acc: 71.99% | Val Acc: 51.63%\n",
            "\n",
            "🎯 Stage 10/35\n",
            "Epoch 1/500 | Train Loss: 0.1901 | Train Acc: 72.05% | Val Acc: 51.60%\n",
            "Epoch 101/500 | Train Loss: 0.1680 | Train Acc: 72.44% | Val Acc: 51.68%\n",
            "Epoch 201/500 | Train Loss: 0.1644 | Train Acc: 72.82% | Val Acc: 51.74%\n",
            "Epoch 301/500 | Train Loss: 0.1623 | Train Acc: 72.98% | Val Acc: 51.71%\n",
            "Epoch 401/500 | Train Loss: 0.1612 | Train Acc: 73.08% | Val Acc: 51.71%\n",
            "Epoch 500/500 | Train Loss: 0.1604 | Train Acc: 73.19% | Val Acc: 51.67%\n",
            "\n",
            "🎯 Stage 11/35\n",
            "Epoch 1/500 | Train Loss: 0.1761 | Train Acc: 73.38% | Val Acc: 51.75%\n",
            "Epoch 101/500 | Train Loss: 0.1622 | Train Acc: 73.91% | Val Acc: 51.57%\n",
            "Epoch 201/500 | Train Loss: 0.1583 | Train Acc: 74.41% | Val Acc: 51.63%\n",
            "Epoch 301/500 | Train Loss: 0.1563 | Train Acc: 74.57% | Val Acc: 51.87%\n",
            "Epoch 401/500 | Train Loss: 0.1555 | Train Acc: 74.73% | Val Acc: 51.58%\n",
            "Epoch 500/500 | Train Loss: 0.1549 | Train Acc: 74.73% | Val Acc: 51.66%\n",
            "\n",
            "🎯 Stage 12/35\n",
            "Epoch 1/500 | Train Loss: 0.1731 | Train Acc: 75.61% | Val Acc: 51.69%\n",
            "Epoch 101/500 | Train Loss: 0.1555 | Train Acc: 76.29% | Val Acc: 51.90%\n",
            "Epoch 201/500 | Train Loss: 0.1516 | Train Acc: 76.73% | Val Acc: 51.79%\n",
            "Epoch 301/500 | Train Loss: 0.1497 | Train Acc: 76.94% | Val Acc: 51.99%\n",
            "Epoch 401/500 | Train Loss: 0.1486 | Train Acc: 76.95% | Val Acc: 51.82%\n",
            "Epoch 500/500 | Train Loss: 0.1479 | Train Acc: 77.04% | Val Acc: 51.89%\n",
            "\n",
            "🎯 Stage 13/35\n",
            "Epoch 1/500 | Train Loss: 0.1685 | Train Acc: 77.74% | Val Acc: 51.73%\n",
            "Epoch 101/500 | Train Loss: 0.1505 | Train Acc: 78.05% | Val Acc: 51.75%\n",
            "Epoch 201/500 | Train Loss: 0.1470 | Train Acc: 78.42% | Val Acc: 51.88%\n",
            "Epoch 301/500 | Train Loss: 0.1452 | Train Acc: 78.54% | Val Acc: 51.84%\n",
            "Epoch 401/500 | Train Loss: 0.1442 | Train Acc: 78.67% | Val Acc: 51.85%\n",
            "Epoch 500/500 | Train Loss: 0.1437 | Train Acc: 78.74% | Val Acc: 51.82%\n",
            "\n",
            "🎯 Stage 14/35\n",
            "Epoch 1/500 | Train Loss: 0.1614 | Train Acc: 78.74% | Val Acc: 51.81%\n",
            "Epoch 101/500 | Train Loss: 0.1446 | Train Acc: 79.13% | Val Acc: 51.82%\n",
            "Epoch 201/500 | Train Loss: 0.1412 | Train Acc: 79.51% | Val Acc: 51.74%\n",
            "Epoch 301/500 | Train Loss: 0.1400 | Train Acc: 79.60% | Val Acc: 51.80%\n",
            "Epoch 401/500 | Train Loss: 0.1393 | Train Acc: 79.65% | Val Acc: 51.69%\n",
            "Epoch 500/500 | Train Loss: 0.1390 | Train Acc: 79.67% | Val Acc: 51.62%\n",
            "\n",
            "🎯 Stage 15/35\n",
            "Epoch 1/500 | Train Loss: 0.1577 | Train Acc: 79.59% | Val Acc: 51.28%\n",
            "Epoch 101/500 | Train Loss: 0.1431 | Train Acc: 80.00% | Val Acc: 51.44%\n",
            "Epoch 201/500 | Train Loss: 0.1394 | Train Acc: 80.42% | Val Acc: 51.43%\n",
            "Epoch 301/500 | Train Loss: 0.1368 | Train Acc: 80.61% | Val Acc: 51.44%\n",
            "Epoch 401/500 | Train Loss: 0.1355 | Train Acc: 80.72% | Val Acc: 51.43%\n",
            "Epoch 500/500 | Train Loss: 0.1348 | Train Acc: 80.78% | Val Acc: 51.38%\n",
            "\n",
            "🎯 Stage 16/35\n",
            "Epoch 1/500 | Train Loss: 0.1494 | Train Acc: 80.72% | Val Acc: 51.65%\n",
            "Epoch 101/500 | Train Loss: 0.1356 | Train Acc: 81.24% | Val Acc: 51.28%\n",
            "Epoch 201/500 | Train Loss: 0.1314 | Train Acc: 81.65% | Val Acc: 51.39%\n",
            "Epoch 301/500 | Train Loss: 0.1296 | Train Acc: 81.80% | Val Acc: 51.28%\n",
            "Epoch 401/500 | Train Loss: 0.1287 | Train Acc: 81.90% | Val Acc: 51.39%\n",
            "Epoch 500/500 | Train Loss: 0.1282 | Train Acc: 81.97% | Val Acc: 51.41%\n",
            "\n",
            "🎯 Stage 17/35\n",
            "Epoch 1/500 | Train Loss: 0.1498 | Train Acc: 82.87% | Val Acc: 51.49%\n",
            "Epoch 101/500 | Train Loss: 0.1317 | Train Acc: 83.56% | Val Acc: 51.44%\n",
            "Epoch 201/500 | Train Loss: 0.1289 | Train Acc: 83.80% | Val Acc: 51.55%\n",
            "Epoch 301/500 | Train Loss: 0.1278 | Train Acc: 83.98% | Val Acc: 51.28%\n",
            "Epoch 401/500 | Train Loss: 0.1273 | Train Acc: 83.99% | Val Acc: 51.34%\n",
            "Epoch 500/500 | Train Loss: 0.1269 | Train Acc: 84.00% | Val Acc: 51.44%\n",
            "\n",
            "🎯 Stage 18/35\n",
            "Epoch 1/500 | Train Loss: 0.1441 | Train Acc: 84.27% | Val Acc: 51.64%\n",
            "Epoch 101/500 | Train Loss: 0.1294 | Train Acc: 84.71% | Val Acc: 51.43%\n",
            "Epoch 201/500 | Train Loss: 0.1260 | Train Acc: 84.95% | Val Acc: 51.33%\n",
            "Epoch 301/500 | Train Loss: 0.1244 | Train Acc: 85.09% | Val Acc: 51.23%\n",
            "Epoch 401/500 | Train Loss: 0.1234 | Train Acc: 85.20% | Val Acc: 51.19%\n",
            "Epoch 500/500 | Train Loss: 0.1228 | Train Acc: 85.25% | Val Acc: 51.21%\n",
            "\n",
            "🎯 Stage 19/35\n",
            "Epoch 1/500 | Train Loss: 0.1386 | Train Acc: 85.20% | Val Acc: 51.42%\n",
            "Epoch 101/500 | Train Loss: 0.1242 | Train Acc: 85.33% | Val Acc: 51.35%\n",
            "Epoch 201/500 | Train Loss: 0.1201 | Train Acc: 85.62% | Val Acc: 51.31%\n",
            "Epoch 301/500 | Train Loss: 0.1183 | Train Acc: 85.78% | Val Acc: 51.48%\n",
            "Epoch 401/500 | Train Loss: 0.1172 | Train Acc: 85.84% | Val Acc: 51.39%\n",
            "Epoch 500/500 | Train Loss: 0.1166 | Train Acc: 85.96% | Val Acc: 51.41%\n",
            "\n",
            "🎯 Stage 20/35\n",
            "Epoch 1/500 | Train Loss: 0.1349 | Train Acc: 85.62% | Val Acc: 51.20%\n",
            "Epoch 101/500 | Train Loss: 0.1210 | Train Acc: 86.05% | Val Acc: 51.17%\n",
            "Epoch 201/500 | Train Loss: 0.1183 | Train Acc: 86.29% | Val Acc: 51.40%\n",
            "Epoch 301/500 | Train Loss: 0.1171 | Train Acc: 86.43% | Val Acc: 51.32%\n",
            "Epoch 401/500 | Train Loss: 0.1163 | Train Acc: 86.45% | Val Acc: 51.39%\n",
            "Epoch 500/500 | Train Loss: 0.1157 | Train Acc: 86.48% | Val Acc: 51.39%\n",
            "\n",
            "🎯 Stage 21/35\n",
            "Epoch 1/500 | Train Loss: 0.1339 | Train Acc: 87.65% | Val Acc: 51.10%\n",
            "Epoch 101/500 | Train Loss: 0.1183 | Train Acc: 87.63% | Val Acc: 51.15%\n",
            "Epoch 201/500 | Train Loss: 0.1157 | Train Acc: 87.87% | Val Acc: 51.20%\n",
            "Epoch 301/500 | Train Loss: 0.1144 | Train Acc: 87.99% | Val Acc: 51.01%\n",
            "Epoch 401/500 | Train Loss: 0.1136 | Train Acc: 88.04% | Val Acc: 50.99%\n",
            "Epoch 500/500 | Train Loss: 0.1130 | Train Acc: 88.10% | Val Acc: 51.06%\n",
            "\n",
            "🎯 Stage 22/35\n",
            "Epoch 1/500 | Train Loss: 0.1280 | Train Acc: 88.33% | Val Acc: 51.07%\n",
            "Epoch 101/500 | Train Loss: 0.1161 | Train Acc: 88.70% | Val Acc: 51.15%\n",
            "Epoch 201/500 | Train Loss: 0.1147 | Train Acc: 88.76% | Val Acc: 51.23%\n",
            "Epoch 301/500 | Train Loss: 0.1137 | Train Acc: 88.86% | Val Acc: 51.15%\n",
            "Epoch 401/500 | Train Loss: 0.1131 | Train Acc: 88.85% | Val Acc: 51.17%\n",
            "Epoch 500/500 | Train Loss: 0.1125 | Train Acc: 88.91% | Val Acc: 51.22%\n",
            "\n",
            "🎯 Stage 23/35\n",
            "Epoch 1/500 | Train Loss: 0.1275 | Train Acc: 88.48% | Val Acc: 50.65%\n",
            "Epoch 101/500 | Train Loss: 0.1142 | Train Acc: 88.92% | Val Acc: 50.89%\n",
            "Epoch 201/500 | Train Loss: 0.1124 | Train Acc: 89.05% | Val Acc: 51.02%\n",
            "Epoch 301/500 | Train Loss: 0.1112 | Train Acc: 89.13% | Val Acc: 50.99%\n",
            "Epoch 401/500 | Train Loss: 0.1104 | Train Acc: 89.16% | Val Acc: 50.98%\n",
            "Epoch 500/500 | Train Loss: 0.1099 | Train Acc: 89.23% | Val Acc: 50.99%\n",
            "\n",
            "🎯 Stage 24/35\n",
            "Epoch 1/500 | Train Loss: 0.1229 | Train Acc: 89.65% | Val Acc: 51.01%\n",
            "Epoch 101/500 | Train Loss: 0.1099 | Train Acc: 89.64% | Val Acc: 51.03%\n",
            "Epoch 201/500 | Train Loss: 0.1073 | Train Acc: 89.90% | Val Acc: 50.95%\n",
            "Epoch 301/500 | Train Loss: 0.1060 | Train Acc: 89.99% | Val Acc: 51.11%\n",
            "Epoch 401/500 | Train Loss: 0.1055 | Train Acc: 90.01% | Val Acc: 51.16%\n",
            "Epoch 500/500 | Train Loss: 0.1053 | Train Acc: 90.01% | Val Acc: 51.24%\n",
            "\n",
            "🎯 Stage 25/35\n",
            "Epoch 1/500 | Train Loss: 0.1180 | Train Acc: 90.15% | Val Acc: 51.42%\n",
            "Epoch 101/500 | Train Loss: 0.1063 | Train Acc: 90.39% | Val Acc: 51.19%\n",
            "Epoch 201/500 | Train Loss: 0.1042 | Train Acc: 90.59% | Val Acc: 51.37%\n",
            "Epoch 301/500 | Train Loss: 0.1030 | Train Acc: 90.69% | Val Acc: 51.20%\n",
            "Epoch 401/500 | Train Loss: 0.1023 | Train Acc: 90.71% | Val Acc: 51.17%\n",
            "Epoch 500/500 | Train Loss: 0.1019 | Train Acc: 90.78% | Val Acc: 51.29%\n",
            "\n",
            "🎯 Stage 26/35\n",
            "Epoch 1/500 | Train Loss: 0.1139 | Train Acc: 89.86% | Val Acc: 51.50%\n",
            "Epoch 101/500 | Train Loss: 0.1052 | Train Acc: 90.55% | Val Acc: 51.34%\n",
            "Epoch 201/500 | Train Loss: 0.1032 | Train Acc: 90.73% | Val Acc: 51.46%\n",
            "Epoch 301/500 | Train Loss: 0.1017 | Train Acc: 90.86% | Val Acc: 51.34%\n",
            "Epoch 401/500 | Train Loss: 0.1007 | Train Acc: 90.93% | Val Acc: 51.36%\n",
            "Epoch 500/500 | Train Loss: 0.1000 | Train Acc: 90.93% | Val Acc: 51.28%\n",
            "\n",
            "🎯 Stage 27/35\n",
            "Epoch 1/500 | Train Loss: 0.1157 | Train Acc: 92.24% | Val Acc: 51.24%\n",
            "Epoch 101/500 | Train Loss: 0.1021 | Train Acc: 92.34% | Val Acc: 51.11%\n",
            "Epoch 201/500 | Train Loss: 0.1001 | Train Acc: 92.47% | Val Acc: 51.13%\n",
            "Epoch 301/500 | Train Loss: 0.0986 | Train Acc: 92.59% | Val Acc: 51.06%\n",
            "Epoch 401/500 | Train Loss: 0.0978 | Train Acc: 92.61% | Val Acc: 51.09%\n",
            "Epoch 500/500 | Train Loss: 0.0974 | Train Acc: 92.63% | Val Acc: 51.11%\n",
            "\n",
            "🎯 Stage 28/35\n",
            "Epoch 1/500 | Train Loss: 0.1166 | Train Acc: 93.34% | Val Acc: 50.77%\n",
            "Epoch 101/500 | Train Loss: 0.1005 | Train Acc: 93.52% | Val Acc: 51.06%\n",
            "Epoch 201/500 | Train Loss: 0.0991 | Train Acc: 93.62% | Val Acc: 51.03%\n",
            "Epoch 301/500 | Train Loss: 0.0981 | Train Acc: 93.73% | Val Acc: 50.95%\n",
            "Epoch 401/500 | Train Loss: 0.0975 | Train Acc: 93.74% | Val Acc: 51.05%\n",
            "Epoch 500/500 | Train Loss: 0.0970 | Train Acc: 93.77% | Val Acc: 50.99%\n",
            "\n",
            "🎯 Stage 29/35\n",
            "Epoch 1/500 | Train Loss: 0.1107 | Train Acc: 93.92% | Val Acc: 50.98%\n",
            "Epoch 101/500 | Train Loss: 0.0973 | Train Acc: 94.25% | Val Acc: 51.06%\n",
            "Epoch 201/500 | Train Loss: 0.0955 | Train Acc: 94.34% | Val Acc: 50.93%\n",
            "Epoch 301/500 | Train Loss: 0.0946 | Train Acc: 94.43% | Val Acc: 50.88%\n",
            "Epoch 401/500 | Train Loss: 0.0938 | Train Acc: 94.48% | Val Acc: 50.96%\n",
            "Epoch 500/500 | Train Loss: 0.0935 | Train Acc: 94.51% | Val Acc: 50.96%\n",
            "\n",
            "🎯 Stage 30/35\n",
            "Epoch 1/500 | Train Loss: 0.1067 | Train Acc: 94.96% | Val Acc: 51.09%\n",
            "Epoch 101/500 | Train Loss: 0.0958 | Train Acc: 95.11% | Val Acc: 50.98%\n",
            "Epoch 201/500 | Train Loss: 0.0943 | Train Acc: 95.24% | Val Acc: 51.17%\n",
            "Epoch 301/500 | Train Loss: 0.0934 | Train Acc: 95.28% | Val Acc: 51.19%\n",
            "Epoch 401/500 | Train Loss: 0.0928 | Train Acc: 95.30% | Val Acc: 51.21%\n",
            "Epoch 500/500 | Train Loss: 0.0925 | Train Acc: 95.32% | Val Acc: 51.20%\n",
            "\n",
            "🎯 Stage 31/35\n",
            "Epoch 1/500 | Train Loss: 0.1070 | Train Acc: 95.54% | Val Acc: 51.05%\n",
            "Epoch 101/500 | Train Loss: 0.0935 | Train Acc: 95.67% | Val Acc: 51.26%\n",
            "Epoch 201/500 | Train Loss: 0.0912 | Train Acc: 95.79% | Val Acc: 51.00%\n",
            "Epoch 301/500 | Train Loss: 0.0901 | Train Acc: 95.84% | Val Acc: 51.14%\n",
            "Epoch 401/500 | Train Loss: 0.0895 | Train Acc: 95.89% | Val Acc: 51.21%\n",
            "Epoch 500/500 | Train Loss: 0.0891 | Train Acc: 95.92% | Val Acc: 51.07%\n",
            "\n",
            "🎯 Stage 32/35\n",
            "Epoch 1/500 | Train Loss: 0.1078 | Train Acc: 96.12% | Val Acc: 50.96%\n",
            "Epoch 101/500 | Train Loss: 0.0923 | Train Acc: 96.29% | Val Acc: 51.00%\n",
            "Epoch 201/500 | Train Loss: 0.0912 | Train Acc: 96.32% | Val Acc: 50.99%\n",
            "Epoch 301/500 | Train Loss: 0.0901 | Train Acc: 96.35% | Val Acc: 51.04%\n",
            "Epoch 401/500 | Train Loss: 0.0893 | Train Acc: 96.38% | Val Acc: 51.02%\n",
            "Epoch 500/500 | Train Loss: 0.0889 | Train Acc: 96.40% | Val Acc: 50.97%\n",
            "\n",
            "🎯 Stage 33/35\n",
            "Epoch 1/500 | Train Loss: 0.1010 | Train Acc: 96.68% | Val Acc: 51.29%\n",
            "Epoch 101/500 | Train Loss: 0.0886 | Train Acc: 96.87% | Val Acc: 51.37%\n",
            "Epoch 201/500 | Train Loss: 0.0862 | Train Acc: 96.93% | Val Acc: 51.47%\n",
            "Epoch 301/500 | Train Loss: 0.0850 | Train Acc: 97.01% | Val Acc: 51.34%\n",
            "Epoch 401/500 | Train Loss: 0.0842 | Train Acc: 97.02% | Val Acc: 51.35%\n",
            "Epoch 500/500 | Train Loss: 0.0838 | Train Acc: 97.06% | Val Acc: 51.32%\n",
            "\n",
            "🎯 Stage 34/35\n",
            "Epoch 1/500 | Train Loss: 0.1018 | Train Acc: 97.21% | Val Acc: 51.27%\n",
            "Epoch 101/500 | Train Loss: 0.0882 | Train Acc: 97.30% | Val Acc: 51.19%\n",
            "Epoch 201/500 | Train Loss: 0.0872 | Train Acc: 97.35% | Val Acc: 51.14%\n",
            "Epoch 301/500 | Train Loss: 0.0864 | Train Acc: 97.35% | Val Acc: 51.16%\n",
            "Epoch 401/500 | Train Loss: 0.0859 | Train Acc: 97.37% | Val Acc: 51.11%\n",
            "Epoch 500/500 | Train Loss: 0.0858 | Train Acc: 97.39% | Val Acc: 51.10%\n",
            "\n",
            "🎯 Stage 35/35\n",
            "Epoch 1/500 | Train Loss: 0.0965 | Train Acc: 97.39% | Val Acc: 51.31%\n",
            "Epoch 101/500 | Train Loss: 0.0867 | Train Acc: 97.55% | Val Acc: 51.26%\n",
            "Epoch 201/500 | Train Loss: 0.0857 | Train Acc: 97.57% | Val Acc: 51.17%\n",
            "Epoch 301/500 | Train Loss: 0.0848 | Train Acc: 97.61% | Val Acc: 51.32%\n",
            "Epoch 401/500 | Train Loss: 0.0840 | Train Acc: 97.65% | Val Acc: 51.44%\n",
            "Epoch 500/500 | Train Loss: 0.0833 | Train Acc: 97.67% | Val Acc: 51.41%\n",
            "✅ GBNN model saved at: /content/drive/MyDrive/SummerIntern/Puf Models/puf_gbnn_V3/model.pt\n",
            " Training history saved at: /content/drive/MyDrive/SummerIntern/Puf Models/puf_gbnn_V3/training_history.xlsx\n",
            "\n",
            "✅ Test Acc: 51.33% | Test Exact Match: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNJ9ptuQkpBn",
        "outputId": "9430c875-6bd5-4048-e7f9-2efc0449872e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🎯 Stage 1/35\n",
            "Epoch 1/200 | Train Loss: 0.2573 | Train Acc: 50.85% | Val Acc: 50.86%\n",
            "Epoch 101/200 | Train Loss: 0.2290 | Train Acc: 52.98% | Val Acc: 50.69%\n",
            "Epoch 200/200 | Train Loss: 0.2230 | Train Acc: 53.68% | Val Acc: 51.01%\n",
            "\n",
            "🎯 Stage 2/35\n",
            "Epoch 1/200 | Train Loss: 0.2474 | Train Acc: 53.63% | Val Acc: 50.86%\n",
            "Epoch 101/200 | Train Loss: 0.2249 | Train Acc: 55.03% | Val Acc: 50.91%\n",
            "Epoch 200/200 | Train Loss: 0.2182 | Train Acc: 55.84% | Val Acc: 50.91%\n",
            "\n",
            "🎯 Stage 3/35\n",
            "Epoch 1/200 | Train Loss: 0.2391 | Train Acc: 55.72% | Val Acc: 50.93%\n",
            "Epoch 101/200 | Train Loss: 0.2208 | Train Acc: 56.64% | Val Acc: 51.11%\n",
            "Epoch 200/200 | Train Loss: 0.2163 | Train Acc: 57.23% | Val Acc: 51.38%\n",
            "\n",
            "🎯 Stage 4/35\n",
            "Epoch 1/200 | Train Loss: 0.2320 | Train Acc: 57.51% | Val Acc: 51.49%\n",
            "Epoch 101/200 | Train Loss: 0.2102 | Train Acc: 58.99% | Val Acc: 51.54%\n",
            "Epoch 200/200 | Train Loss: 0.2037 | Train Acc: 59.72% | Val Acc: 51.67%\n",
            "\n",
            "🎯 Stage 5/35\n",
            "Epoch 1/200 | Train Loss: 0.2236 | Train Acc: 59.82% | Val Acc: 51.74%\n",
            "Epoch 101/200 | Train Loss: 0.2041 | Train Acc: 60.95% | Val Acc: 51.68%\n",
            "Epoch 200/200 | Train Loss: 0.1990 | Train Acc: 61.59% | Val Acc: 51.81%\n",
            "\n",
            "🎯 Stage 6/35\n",
            "Epoch 1/200 | Train Loss: 0.2185 | Train Acc: 61.66% | Val Acc: 51.94%\n",
            "Epoch 101/200 | Train Loss: 0.1984 | Train Acc: 62.90% | Val Acc: 51.97%\n",
            "Epoch 200/200 | Train Loss: 0.1931 | Train Acc: 63.43% | Val Acc: 51.96%\n",
            "\n",
            "🎯 Stage 7/35\n",
            "Epoch 1/200 | Train Loss: 0.2146 | Train Acc: 64.44% | Val Acc: 51.63%\n",
            "Epoch 101/200 | Train Loss: 0.1973 | Train Acc: 64.78% | Val Acc: 51.73%\n",
            "Epoch 200/200 | Train Loss: 0.1934 | Train Acc: 65.23% | Val Acc: 51.64%\n",
            "\n",
            "🎯 Stage 8/35\n",
            "Epoch 1/200 | Train Loss: 0.2054 | Train Acc: 65.47% | Val Acc: 51.67%\n",
            "Epoch 101/200 | Train Loss: 0.1853 | Train Acc: 66.61% | Val Acc: 51.53%\n",
            "Epoch 200/200 | Train Loss: 0.1797 | Train Acc: 67.26% | Val Acc: 51.60%\n",
            "\n",
            "🎯 Stage 9/35\n",
            "Epoch 1/200 | Train Loss: 0.1993 | Train Acc: 67.31% | Val Acc: 51.88%\n",
            "Epoch 101/200 | Train Loss: 0.1829 | Train Acc: 68.00% | Val Acc: 51.75%\n",
            "Epoch 200/200 | Train Loss: 0.1777 | Train Acc: 68.56% | Val Acc: 51.61%\n",
            "\n",
            "🎯 Stage 10/35\n",
            "Epoch 1/200 | Train Loss: 0.1990 | Train Acc: 68.65% | Val Acc: 51.84%\n",
            "Epoch 101/200 | Train Loss: 0.1792 | Train Acc: 69.44% | Val Acc: 51.63%\n",
            "Epoch 200/200 | Train Loss: 0.1753 | Train Acc: 69.86% | Val Acc: 51.63%\n",
            "\n",
            "🎯 Stage 11/35\n",
            "Epoch 1/200 | Train Loss: 0.1921 | Train Acc: 70.11% | Val Acc: 51.72%\n",
            "Epoch 101/200 | Train Loss: 0.1728 | Train Acc: 70.82% | Val Acc: 51.91%\n",
            "Epoch 200/200 | Train Loss: 0.1677 | Train Acc: 71.45% | Val Acc: 51.58%\n",
            "\n",
            "🎯 Stage 12/35\n",
            "Epoch 1/200 | Train Loss: 0.1907 | Train Acc: 71.04% | Val Acc: 51.86%\n",
            "Epoch 101/200 | Train Loss: 0.1697 | Train Acc: 71.46% | Val Acc: 51.93%\n",
            "Epoch 200/200 | Train Loss: 0.1655 | Train Acc: 71.86% | Val Acc: 51.98%\n",
            "\n",
            "🎯 Stage 13/35\n",
            "Epoch 1/200 | Train Loss: 0.1815 | Train Acc: 73.62% | Val Acc: 51.92%\n",
            "Epoch 101/200 | Train Loss: 0.1625 | Train Acc: 74.00% | Val Acc: 52.01%\n",
            "Epoch 200/200 | Train Loss: 0.1563 | Train Acc: 74.59% | Val Acc: 51.96%\n",
            "\n",
            "🎯 Stage 14/35\n",
            "Epoch 1/200 | Train Loss: 0.1738 | Train Acc: 74.60% | Val Acc: 51.82%\n",
            "Epoch 101/200 | Train Loss: 0.1591 | Train Acc: 75.58% | Val Acc: 51.89%\n",
            "Epoch 200/200 | Train Loss: 0.1543 | Train Acc: 76.03% | Val Acc: 51.84%\n",
            "\n",
            "🎯 Stage 15/35\n",
            "Epoch 1/200 | Train Loss: 0.1698 | Train Acc: 75.87% | Val Acc: 51.56%\n",
            "Epoch 101/200 | Train Loss: 0.1566 | Train Acc: 76.59% | Val Acc: 51.57%\n",
            "Epoch 200/200 | Train Loss: 0.1544 | Train Acc: 76.76% | Val Acc: 51.56%\n",
            "\n",
            "🎯 Stage 16/35\n",
            "Epoch 1/200 | Train Loss: 0.1702 | Train Acc: 77.03% | Val Acc: 51.96%\n",
            "Epoch 101/200 | Train Loss: 0.1547 | Train Acc: 77.21% | Val Acc: 51.79%\n",
            "Epoch 200/200 | Train Loss: 0.1518 | Train Acc: 77.49% | Val Acc: 51.79%\n",
            "\n",
            "🎯 Stage 17/35\n",
            "Epoch 1/200 | Train Loss: 0.1700 | Train Acc: 77.72% | Val Acc: 51.81%\n",
            "Epoch 101/200 | Train Loss: 0.1519 | Train Acc: 77.64% | Val Acc: 51.91%\n",
            "Epoch 200/200 | Train Loss: 0.1484 | Train Acc: 77.94% | Val Acc: 51.94%\n",
            "\n",
            "🎯 Stage 18/35\n",
            "Epoch 1/200 | Train Loss: 0.1614 | Train Acc: 77.84% | Val Acc: 51.57%\n",
            "Epoch 101/200 | Train Loss: 0.1490 | Train Acc: 78.33% | Val Acc: 51.80%\n",
            "Epoch 200/200 | Train Loss: 0.1467 | Train Acc: 78.60% | Val Acc: 51.75%\n",
            "\n",
            "🎯 Stage 19/35\n",
            "Epoch 1/200 | Train Loss: 0.1625 | Train Acc: 77.86% | Val Acc: 51.84%\n",
            "Epoch 101/200 | Train Loss: 0.1475 | Train Acc: 78.26% | Val Acc: 51.93%\n",
            "Epoch 200/200 | Train Loss: 0.1451 | Train Acc: 78.47% | Val Acc: 51.79%\n",
            "\n",
            "🎯 Stage 20/35\n",
            "Epoch 1/200 | Train Loss: 0.1582 | Train Acc: 78.00% | Val Acc: 51.79%\n",
            "Epoch 101/200 | Train Loss: 0.1433 | Train Acc: 78.45% | Val Acc: 52.00%\n",
            "Epoch 200/200 | Train Loss: 0.1405 | Train Acc: 78.62% | Val Acc: 52.00%\n",
            "\n",
            "🎯 Stage 21/35\n",
            "Epoch 1/200 | Train Loss: 0.1533 | Train Acc: 79.13% | Val Acc: 52.15%\n",
            "Epoch 101/200 | Train Loss: 0.1409 | Train Acc: 79.44% | Val Acc: 52.09%\n",
            "Epoch 200/200 | Train Loss: 0.1379 | Train Acc: 79.75% | Val Acc: 52.31%\n",
            "\n",
            "🎯 Stage 22/35\n",
            "Epoch 1/200 | Train Loss: 0.1534 | Train Acc: 80.08% | Val Acc: 52.00%\n",
            "Epoch 101/200 | Train Loss: 0.1384 | Train Acc: 80.79% | Val Acc: 51.99%\n",
            "Epoch 200/200 | Train Loss: 0.1356 | Train Acc: 81.05% | Val Acc: 51.95%\n",
            "\n",
            "🎯 Stage 23/35\n",
            "Epoch 1/200 | Train Loss: 0.1469 | Train Acc: 81.82% | Val Acc: 52.11%\n",
            "Epoch 101/200 | Train Loss: 0.1355 | Train Acc: 82.25% | Val Acc: 51.99%\n",
            "Epoch 200/200 | Train Loss: 0.1331 | Train Acc: 82.44% | Val Acc: 52.07%\n",
            "\n",
            "🎯 Stage 24/35\n",
            "Epoch 1/200 | Train Loss: 0.1472 | Train Acc: 82.46% | Val Acc: 52.08%\n",
            "Epoch 101/200 | Train Loss: 0.1331 | Train Acc: 82.81% | Val Acc: 51.98%\n",
            "Epoch 200/200 | Train Loss: 0.1288 | Train Acc: 83.21% | Val Acc: 52.23%\n",
            "\n",
            "🎯 Stage 25/35\n",
            "Epoch 1/200 | Train Loss: 0.1446 | Train Acc: 84.26% | Val Acc: 52.19%\n",
            "Epoch 101/200 | Train Loss: 0.1309 | Train Acc: 84.42% | Val Acc: 52.14%\n",
            "Epoch 200/200 | Train Loss: 0.1276 | Train Acc: 84.67% | Val Acc: 52.24%\n",
            "\n",
            "🎯 Stage 26/35\n",
            "Epoch 1/200 | Train Loss: 0.1477 | Train Acc: 84.77% | Val Acc: 51.86%\n",
            "Epoch 101/200 | Train Loss: 0.1296 | Train Acc: 85.64% | Val Acc: 52.08%\n",
            "Epoch 200/200 | Train Loss: 0.1275 | Train Acc: 85.80% | Val Acc: 52.13%\n",
            "\n",
            "🎯 Stage 27/35\n",
            "Epoch 1/200 | Train Loss: 0.1472 | Train Acc: 86.77% | Val Acc: 51.92%\n",
            "Epoch 101/200 | Train Loss: 0.1288 | Train Acc: 86.89% | Val Acc: 51.83%\n",
            "Epoch 200/200 | Train Loss: 0.1274 | Train Acc: 87.02% | Val Acc: 51.78%\n",
            "\n",
            "🎯 Stage 28/35\n",
            "Epoch 1/200 | Train Loss: 0.1413 | Train Acc: 86.93% | Val Acc: 52.27%\n",
            "Epoch 101/200 | Train Loss: 0.1265 | Train Acc: 87.08% | Val Acc: 52.11%\n",
            "Epoch 200/200 | Train Loss: 0.1238 | Train Acc: 87.34% | Val Acc: 51.99%\n",
            "\n",
            "🎯 Stage 29/35\n",
            "Epoch 1/200 | Train Loss: 0.1411 | Train Acc: 88.47% | Val Acc: 52.27%\n",
            "Epoch 101/200 | Train Loss: 0.1249 | Train Acc: 88.75% | Val Acc: 52.13%\n",
            "Epoch 200/200 | Train Loss: 0.1227 | Train Acc: 88.95% | Val Acc: 52.03%\n",
            "\n",
            "🎯 Stage 30/35\n",
            "Epoch 1/200 | Train Loss: 0.1392 | Train Acc: 89.60% | Val Acc: 51.77%\n",
            "Epoch 101/200 | Train Loss: 0.1227 | Train Acc: 89.78% | Val Acc: 51.88%\n",
            "Epoch 200/200 | Train Loss: 0.1204 | Train Acc: 89.95% | Val Acc: 51.76%\n",
            "\n",
            "🎯 Stage 31/35\n",
            "Epoch 1/200 | Train Loss: 0.1352 | Train Acc: 90.31% | Val Acc: 51.97%\n",
            "Epoch 101/200 | Train Loss: 0.1208 | Train Acc: 90.57% | Val Acc: 51.84%\n",
            "Epoch 200/200 | Train Loss: 0.1183 | Train Acc: 90.78% | Val Acc: 51.81%\n",
            "\n",
            "🎯 Stage 32/35\n",
            "Epoch 1/200 | Train Loss: 0.1293 | Train Acc: 90.95% | Val Acc: 52.00%\n",
            "Epoch 101/200 | Train Loss: 0.1189 | Train Acc: 91.31% | Val Acc: 51.72%\n",
            "Epoch 200/200 | Train Loss: 0.1168 | Train Acc: 91.50% | Val Acc: 52.01%\n",
            "\n",
            "🎯 Stage 33/35\n",
            "Epoch 1/200 | Train Loss: 0.1293 | Train Acc: 91.50% | Val Acc: 51.66%\n",
            "Epoch 101/200 | Train Loss: 0.1175 | Train Acc: 92.18% | Val Acc: 51.74%\n",
            "Epoch 200/200 | Train Loss: 0.1156 | Train Acc: 92.30% | Val Acc: 51.53%\n",
            "\n",
            "🎯 Stage 34/35\n",
            "Epoch 1/200 | Train Loss: 0.1276 | Train Acc: 92.64% | Val Acc: 51.67%\n",
            "Epoch 101/200 | Train Loss: 0.1159 | Train Acc: 92.85% | Val Acc: 51.86%\n",
            "Epoch 200/200 | Train Loss: 0.1137 | Train Acc: 93.01% | Val Acc: 51.90%\n",
            "\n",
            "🎯 Stage 35/35\n",
            "Epoch 1/200 | Train Loss: 0.1297 | Train Acc: 93.27% | Val Acc: 51.93%\n",
            "Epoch 101/200 | Train Loss: 0.1146 | Train Acc: 93.53% | Val Acc: 51.88%\n",
            "Epoch 200/200 | Train Loss: 0.1124 | Train Acc: 93.64% | Val Acc: 51.94%\n",
            "\n",
            "✅ Test Acc: 51.97% | Test Exact Match: 0.00%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(GBNN(\n",
              "   (models): ModuleList(\n",
              "     (0-34): 35 x MultiOutputNN(\n",
              "       (model): Sequential(\n",
              "         (0): Linear(in_features=32, out_features=64, bias=True)\n",
              "         (1): ReLU()\n",
              "         (2): Linear(in_features=64, out_features=64, bias=True)\n",
              "         (3): ReLU()\n",
              "         (4): Linear(in_features=64, out_features=32, bias=True)\n",
              "       )\n",
              "     )\n",
              "   )\n",
              " ),\n",
              " [[1, 1, 0.5085446238517761, 0.5086249709129333, 0.25726503133773804],\n",
              "  [1, 2, 0.5094553828239441, 0.5086874961853027, 0.25095322728157043],\n",
              "  [1, 3, 0.5090267658233643, 0.5068749785423279, 0.24902571737766266],\n",
              "  [1, 4, 0.5086339116096497, 0.5062500238418579, 0.24778789281845093],\n",
              "  [1, 5, 0.5086517930030823, 0.5061874985694885, 0.24748294055461884],\n",
              "  [1, 6, 0.5086785554885864, 0.5056874752044678, 0.24736183881759644],\n",
              "  [1, 7, 0.5089107155799866, 0.5058125257492065, 0.24722430109977722],\n",
              "  [1, 8, 0.5093035697937012, 0.5057500004768372, 0.24707019329071045],\n",
              "  [1, 9, 0.5096339583396912, 0.5068125128746033, 0.24690544605255127],\n",
              "  [1, 10, 0.5099285840988159, 0.507437527179718, 0.24673189222812653],\n",
              "  [1, 11, 0.510232150554657, 0.5069375038146973, 0.24658340215682983],\n",
              "  [1, 12, 0.5102678537368774, 0.5074999928474426, 0.24644601345062256],\n",
              "  [1, 13, 0.510357141494751, 0.5077499747276306, 0.24631506204605103],\n",
              "  [1, 14, 0.510410726070404, 0.507937490940094, 0.2461967021226883],\n",
              "  [1, 15, 0.5104285478591919, 0.508062481880188, 0.24605143070220947],\n",
              "  [1, 16, 0.5105446577072144, 0.5083125233650208, 0.24589551985263824],\n",
              "  [1, 17, 0.5109553337097168, 0.5081250071525574, 0.24575066566467285],\n",
              "  [1, 18, 0.5110982060432434, 0.5078125, 0.24559740722179413],\n",
              "  [1, 19, 0.5110803842544556, 0.5078125, 0.24544154107570648],\n",
              "  [1, 20, 0.5112946629524231, 0.507437527179718, 0.24527134001255035],\n",
              "  [1, 21, 0.5116249918937683, 0.5070000290870667, 0.2451006919145584],\n",
              "  [1, 22, 0.511696457862854, 0.5069375038146973, 0.24493131041526794],\n",
              "  [1, 23, 0.5119107365608215, 0.5069375038146973, 0.2447466403245926],\n",
              "  [1, 24, 0.5122767686843872, 0.5066875219345093, 0.24456359446048737],\n",
              "  [1, 25, 0.512776792049408, 0.5071874856948853, 0.24436071515083313],\n",
              "  [1, 26, 0.5128660798072815, 0.5069375038146973, 0.24416562914848328],\n",
              "  [1, 27, 0.5129910707473755, 0.5071874856948853, 0.24395878612995148],\n",
              "  [1, 28, 0.5131160616874695, 0.5067499876022339, 0.24374191462993622],\n",
              "  [1, 29, 0.5131607055664062, 0.5058749914169312, 0.24351413547992706],\n",
              "  [1, 30, 0.5132856965065002, 0.5058749914169312, 0.2432829588651657],\n",
              "  [1, 31, 0.5135178565979004, 0.5054374933242798, 0.24303612112998962],\n",
              "  [1, 32, 0.5141071677207947, 0.5058125257492065, 0.24280200898647308],\n",
              "  [1, 33, 0.5141696333885193, 0.5061874985694885, 0.24255861341953278],\n",
              "  [1, 34, 0.5143839120864868, 0.5056874752044678, 0.24230964481830597],\n",
              "  [1, 35, 0.5147143006324768, 0.5060625076293945, 0.2420627623796463],\n",
              "  [1, 36, 0.5148482322692871, 0.5061249732971191, 0.24181100726127625],\n",
              "  [1, 37, 0.5151249766349792, 0.5056874752044678, 0.24156513810157776],\n",
              "  [1, 38, 0.5155982375144958, 0.5056874752044678, 0.24132125079631805],\n",
              "  [1, 39, 0.5156517624855042, 0.5051875114440918, 0.2410963624715805],\n",
              "  [1, 40, 0.5162231922149658, 0.5061249732971191, 0.24090874195098877],\n",
              "  [1, 41, 0.5161607265472412, 0.5056874752044678, 0.240761861205101],\n",
              "  [1, 42, 0.5165982246398926, 0.5061874985694885, 0.240462988615036],\n",
              "  [1, 43, 0.5167678594589233, 0.5060625076293945, 0.24022246897220612],\n",
              "  [1, 44, 0.5168571472167969, 0.5053125023841858, 0.2400825172662735],\n",
              "  [1, 45, 0.5176964402198792, 0.5057500004768372, 0.23974855244159698],\n",
              "  [1, 46, 0.5177321434020996, 0.5063124895095825, 0.2395477592945099],\n",
              "  [1, 47, 0.5178124904632568, 0.5061874985694885, 0.23930080235004425],\n",
              "  [1, 48, 0.5183928608894348, 0.5061249732971191, 0.23904000222682953],\n",
              "  [1, 49, 0.5187857151031494, 0.5059375166893005, 0.23883222043514252],\n",
              "  [1, 50, 0.5192499756813049, 0.5053750276565552, 0.23861415684223175],\n",
              "  [1, 51, 0.5194643139839172, 0.5063750147819519, 0.23832693696022034],\n",
              "  [1, 52, 0.5196696519851685, 0.5056250095367432, 0.23816195130348206],\n",
              "  [1, 53, 0.5201964378356934, 0.5056874752044678, 0.2378513216972351],\n",
              "  [1, 54, 0.5196607112884521, 0.5066249966621399, 0.237722247838974],\n",
              "  [1, 55, 0.5198660492897034, 0.5061874985694885, 0.23746132850646973],\n",
              "  [1, 56, 0.520767867565155, 0.5059375166893005, 0.23719435930252075],\n",
              "  [1, 57, 0.520892858505249, 0.5064374804496765, 0.23700357973575592],\n",
              "  [1, 58, 0.5208660960197449, 0.5066249966621399, 0.23670953512191772],\n",
              "  [1, 59, 0.5212857127189636, 0.5066875219345093, 0.2364984005689621],\n",
              "  [1, 60, 0.5215178728103638, 0.5059999823570251, 0.23629820346832275],\n",
              "  [1, 61, 0.5221696496009827, 0.5068749785423279, 0.23614078760147095],\n",
              "  [1, 62, 0.5217499732971191, 0.5063750147819519, 0.23590730130672455],\n",
              "  [1, 63, 0.5223660469055176, 0.5066249966621399, 0.2356061339378357],\n",
              "  [1, 64, 0.5229374766349792, 0.5068749785423279, 0.23542818427085876],\n",
              "  [1, 65, 0.522857129573822, 0.5059999823570251, 0.2352820485830307],\n",
              "  [1, 66, 0.5229732394218445, 0.5068749785423279, 0.23499852418899536],\n",
              "  [1, 67, 0.5238392949104309, 0.5073750019073486, 0.23475126922130585],\n",
              "  [1, 68, 0.5231518149375916, 0.507437527179718, 0.23458515107631683],\n",
              "  [1, 69, 0.5236250162124634, 0.5070624947547913, 0.23437553644180298],\n",
              "  [1, 70, 0.5239821672439575, 0.5073750019073486, 0.23415915668010712],\n",
              "  [1, 71, 0.5242321491241455, 0.5066875219345093, 0.2340267151594162],\n",
              "  [1, 72, 0.5240356922149658, 0.507937490940094, 0.23380480706691742],\n",
              "  [1, 73, 0.5246160626411438, 0.5071874856948853, 0.2336241602897644],\n",
              "  [1, 74, 0.5245803594589233, 0.5080000162124634, 0.23339886963367462],\n",
              "  [1, 75, 0.5254285931587219, 0.5078750252723694, 0.23323246836662292],\n",
              "  [1, 76, 0.5252589583396912, 0.5073124766349792, 0.23299217224121094],\n",
              "  [1, 77, 0.5255089402198792, 0.5080000162124634, 0.23285360634326935],\n",
              "  [1, 78, 0.5259553790092468, 0.5067499876022339, 0.23267731070518494],\n",
              "  [1, 79, 0.5258035659790039, 0.507687509059906, 0.23244594037532806],\n",
              "  [1, 80, 0.5258392691612244, 0.5076249837875366, 0.2322816401720047],\n",
              "  [1, 81, 0.526285707950592, 0.5068125128746033, 0.2321140468120575],\n",
              "  [1, 82, 0.526232123374939, 0.507437527179718, 0.2318984419107437],\n",
              "  [1, 83, 0.5266071557998657, 0.5061874985694885, 0.23170654475688934],\n",
              "  [1, 84, 0.5264285802841187, 0.5072500109672546, 0.23162874579429626],\n",
              "  [1, 85, 0.5272678732872009, 0.5073750019073486, 0.23141825199127197],\n",
              "  [1, 86, 0.5269553661346436, 0.507687509059906, 0.2312626838684082],\n",
              "  [1, 87, 0.5272856950759888, 0.5063750147819519, 0.2311553657054901],\n",
              "  [1, 88, 0.5273928642272949, 0.5080000162124634, 0.23096418380737305],\n",
              "  [1, 89, 0.5276428461074829, 0.5071250200271606, 0.23079469799995422],\n",
              "  [1, 90, 0.5277410745620728, 0.5068749785423279, 0.23059912025928497],\n",
              "  [1, 91, 0.5282857418060303, 0.507937490940094, 0.2305021733045578],\n",
              "  [1, 92, 0.528526782989502, 0.5073750019073486, 0.230331689119339],\n",
              "  [1, 93, 0.5279374718666077, 0.5078750252723694, 0.23019367456436157],\n",
              "  [1, 94, 0.5285089015960693, 0.5067499876022339, 0.2300482541322708],\n",
              "  [1, 95, 0.5291785597801208, 0.507437527179718, 0.22984926402568817],\n",
              "  [1, 96, 0.5290178656578064, 0.5076249837875366, 0.22972136735916138],\n",
              "  [1, 97, 0.5289642810821533, 0.5078125, 0.22961314022541046],\n",
              "  [1, 98, 0.5290356874465942, 0.507437527179718, 0.22941534221172333],\n",
              "  [1, 99, 0.5299285650253296, 0.5084375143051147, 0.22931912541389465],\n",
              "  [1, 100, 0.5292589068412781, 0.5080000162124634, 0.22913886606693268],\n",
              "  [1, 101, 0.5297678709030151, 0.5069375038146973, 0.2290031760931015],\n",
              "  [1, 102, 0.5300714373588562, 0.5084999799728394, 0.2288561910390854],\n",
              "  [1, 103, 0.5295714139938354, 0.5066249966621399, 0.22878409922122955],\n",
              "  [1, 104, 0.5305356979370117, 0.5096250176429749, 0.22870053350925446],\n",
              "  [1, 105, 0.5299464464187622, 0.5078125, 0.22866500914096832],\n",
              "  [1, 106, 0.5299553275108337, 0.508187472820282, 0.22853043675422668],\n",
              "  [1, 107, 0.5308839082717896, 0.5084999799728394, 0.22832328081130981],\n",
              "  [1, 108, 0.5305089354515076, 0.5088750123977661, 0.22816325724124908],\n",
              "  [1, 109, 0.5308303833007812, 0.5096250176429749, 0.2280682772397995],\n",
              "  [1, 110, 0.5306071639060974, 0.507562518119812, 0.22797757387161255],\n",
              "  [1, 111, 0.5309910774230957, 0.5083749890327454, 0.22796215116977692],\n",
              "  [1, 112, 0.5312053561210632, 0.508187472820282, 0.22780735790729523],\n",
              "  [1, 113, 0.5315178632736206, 0.5084375143051147, 0.22761577367782593],\n",
              "  [1, 114, 0.5318660736083984, 0.5090625286102295, 0.2275305986404419],\n",
              "  [1, 115, 0.5318124890327454, 0.5073124766349792, 0.22747044265270233],\n",
              "  [1, 116, 0.5319643020629883, 0.5092499852180481, 0.22743022441864014],\n",
              "  [1, 117, 0.531624972820282, 0.508187472820282, 0.22734655439853668],\n",
              "  [1, 118, 0.5321875214576721, 0.5076249837875366, 0.2272893190383911],\n",
              "  [1, 119, 0.5314196348190308, 0.5081250071525574, 0.22712434828281403],\n",
              "  [1, 120, 0.5323125123977661, 0.5073750019073486, 0.22707702219486237],\n",
              "  [1, 121, 0.5322142839431763, 0.508187472820282, 0.22704961895942688],\n",
              "  [1, 122, 0.5321160554885864, 0.5065000057220459, 0.22700472176074982],\n",
              "  [1, 123, 0.5324553847312927, 0.507687509059906, 0.22675779461860657],\n",
              "  [1, 124, 0.53220534324646, 0.5078125, 0.2267332822084427],\n",
              "  [1, 125, 0.5332499742507935, 0.5078125, 0.22667472064495087],\n",
              "  [1, 126, 0.5319643020629883, 0.5073750019073486, 0.2265852689743042],\n",
              "  [1, 127, 0.5333303809165955, 0.507437527179718, 0.22659772634506226],\n",
              "  [1, 128, 0.5327946543693542, 0.5068749785423279, 0.22643588483333588],\n",
              "  [1, 129, 0.5325624942779541, 0.5066249966621399, 0.22632110118865967],\n",
              "  [1, 130, 0.5334643125534058, 0.507937490940094, 0.22637273371219635],\n",
              "  [1, 131, 0.5323214530944824, 0.507437527179718, 0.2263985425233841],\n",
              "  [1, 132, 0.5327678322792053, 0.5059999823570251, 0.22637593746185303],\n",
              "  [1, 133, 0.5338660478591919, 0.5078125, 0.22613395750522614],\n",
              "  [1, 134, 0.5327767729759216, 0.5065624713897705, 0.2261326014995575],\n",
              "  [1, 135, 0.533419668674469, 0.5065624713897705, 0.22599400579929352],\n",
              "  [1, 136, 0.5335714221000671, 0.5069375038146973, 0.22584424912929535],\n",
              "  [1, 137, 0.5331339240074158, 0.5068125128746033, 0.22579780220985413],\n",
              "  [1, 138, 0.5340535640716553, 0.5070624947547913, 0.22579282522201538],\n",
              "  [1, 139, 0.5331160426139832, 0.5077499747276306, 0.22566404938697815],\n",
              "  [1, 140, 0.5333928465843201, 0.5068749785423279, 0.22563239932060242],\n",
              "  [1, 141, 0.5340803861618042, 0.5065624713897705, 0.22548821568489075],\n",
              "  [1, 142, 0.5336339473724365, 0.5070624947547913, 0.22544623911380768],\n",
              "  [1, 143, 0.5335000157356262, 0.5065624713897705, 0.22541560232639313],\n",
              "  [1, 144, 0.5335893034934998, 0.507562518119812, 0.22531914710998535],\n",
              "  [1, 145, 0.5336250066757202, 0.5070624947547913, 0.22529436647891998],\n",
              "  [1, 146, 0.5338303446769714, 0.5074999928474426, 0.22523456811904907],\n",
              "  [1, 147, 0.5342767834663391, 0.5073750019073486, 0.22521057724952698],\n",
              "  [1, 148, 0.533669650554657, 0.5070000290870667, 0.2251211255788803],\n",
              "  [1, 149, 0.53434818983078, 0.5073750019073486, 0.2250586599111557],\n",
              "  [1, 150, 0.5341785550117493, 0.5073124766349792, 0.22500139474868774],\n",
              "  [1, 151, 0.5341517925262451, 0.5068125128746033, 0.22494561970233917],\n",
              "  [1, 152, 0.5339106917381287, 0.5078125, 0.22485271096229553],\n",
              "  [1, 153, 0.5342857241630554, 0.508187472820282, 0.22476644814014435],\n",
              "  [1, 154, 0.5341607332229614, 0.507437527179718, 0.22469188272953033],\n",
              "  [1, 155, 0.5342589020729065, 0.5067499876022339, 0.22464248538017273],\n",
              "  [1, 156, 0.5343303680419922, 0.5083125233650208, 0.22460606694221497],\n",
              "  [1, 157, 0.5347232222557068, 0.5066249966621399, 0.2245926558971405],\n",
              "  [1, 158, 0.5340714454650879, 0.5083125233650208, 0.2246914952993393],\n",
              "  [1, 159, 0.534223198890686, 0.507437527179718, 0.2249235361814499],\n",
              "  [1, 160, 0.534500002861023, 0.5086874961853027, 0.224846750497818],\n",
              "  [1, 161, 0.5344910621643066, 0.5078750252723694, 0.22466017305850983],\n",
              "  [1, 162, 0.5350267887115479, 0.5078125, 0.22445787489414215],\n",
              "  [1, 163, 0.5350089073181152, 0.5088750123977661, 0.2244309037923813],\n",
              "  [1, 164, 0.5349196195602417, 0.507437527179718, 0.22457151114940643],\n",
              "  [1, 165, 0.5352410674095154, 0.5088124871253967, 0.22442841529846191],\n",
              "  [1, 166, 0.5346339344978333, 0.5088750123977661, 0.22427386045455933],\n",
              "  [1, 167, 0.5353124737739563, 0.5086874961853027, 0.2242671251296997],\n",
              "  [1, 168, 0.5349017977714539, 0.5083125233650208, 0.22426898777484894],\n",
              "  [1, 169, 0.5352678298950195, 0.5084999799728394, 0.22416554391384125],\n",
              "  [1, 170, 0.5354910492897034, 0.5094375014305115, 0.22407613694667816],\n",
              "  [1, 171, 0.5347053408622742, 0.5090625286102295, 0.2241073101758957],\n",
              "  [1, 172, 0.5356696248054504, 0.5091875195503235, 0.224171444773674],\n",
              "  [1, 173, 0.5354375243186951, 0.5080000162124634, 0.22398696839809418],\n",
              "  [1, 174, 0.5352857112884521, 0.5098749995231628, 0.2240624576807022],\n",
              "  [1, 175, 0.5351428389549255, 0.5085625052452087, 0.22401723265647888],\n",
              "  [1, 176, 0.536062479019165, 0.5094375014305115, 0.22387918829917908],\n",
              "  [1, 177, 0.5354999899864197, 0.5092499852180481, 0.22389931976795197],\n",
              "  [1, 178, 0.5357410907745361, 0.5089374780654907, 0.2237606793642044],\n",
              "  [1, 179, 0.5356428623199463, 0.5093125104904175, 0.22370268404483795],\n",
              "  [1, 180, 0.5353303551673889, 0.5099999904632568, 0.22372323274612427],\n",
              "  [1, 181, 0.5355982184410095, 0.5104374885559082, 0.22361725568771362],\n",
              "  [1, 182, 0.5364106893539429, 0.5105624794960022, 0.223552867770195],\n",
              "  [1, 183, 0.5356428623199463, 0.5100625157356262, 0.22354839742183685],\n",
              "  [1, 184, 0.5356964468955994, 0.5098749995231628, 0.22354070842266083],\n",
              "  [1, 185, 0.5356964468955994, 0.5101875066757202, 0.22358138859272003],\n",
              "  [1, 186, 0.5358571410179138, 0.5098749995231628, 0.2235892117023468],\n",
              "  [1, 187, 0.5355714559555054, 0.5098124742507935, 0.22356437146663666],\n",
              "  [1, 188, 0.5360714197158813, 0.5090000033378601, 0.22351984679698944],\n",
              "  [1, 189, 0.536062479019165, 0.5098749995231628, 0.2233639806509018],\n",
              "  [1, 190, 0.5357767939567566, 0.5102499723434448, 0.22330425679683685],\n",
              "  [1, 191, 0.536517858505249, 0.5097500085830688, 0.22325628995895386],\n",
              "  [1, 192, 0.5355446338653564, 0.5088750123977661, 0.22323599457740784],\n",
              "  [1, 193, 0.5363482236862183, 0.5091249942779541, 0.22323453426361084],\n",
              "  [1, 194, 0.5360714197158813, 0.5092499852180481, 0.22321690618991852],\n",
              "  [1, 195, 0.536464273929596, 0.5098124742507935, 0.2231665551662445],\n",
              "  [1, 196, 0.536062479019165, 0.5099999904632568, 0.22317872941493988],\n",
              "  [1, 197, 0.5358214378356934, 0.5095000267028809, 0.22313950955867767],\n",
              "  [1, 198, 0.5367143154144287, 0.5099999904632568, 0.22307942807674408],\n",
              "  [1, 199, 0.5359017848968506, 0.5101875066757202, 0.22305816411972046],\n",
              "  [1, 200, 0.536767840385437, 0.5101249814033508, 0.2230413854122162],\n",
              "  [2, 1, 0.5363125205039978, 0.5086249709129333, 0.24735553562641144],\n",
              "  [2, 2, 0.5353660583496094, 0.5078750252723694, 0.23906980454921722],\n",
              "  [2, 3, 0.5361517667770386, 0.5088124871253967, 0.2381141036748886],\n",
              "  [2, 4, 0.5364106893539429, 0.508062481880188, 0.2373548001050949],\n",
              "  [2, 5, 0.5364464521408081, 0.507937490940094, 0.23705284297466278],\n",
              "  [2, 6, 0.5363660454750061, 0.5083749890327454, 0.23694439232349396],\n",
              "  [2, 7, 0.5361250042915344, 0.5086249709129333, 0.23685164749622345],\n",
              "  [2, 8, 0.5361250042915344, 0.5088750123977661, 0.23680298030376434],\n",
              "  [2, 9, 0.5362321138381958, 0.5088124871253967, 0.23673869669437408],\n",
              "  [2, 10, 0.5362499952316284, 0.508062481880188, 0.23664705455303192],\n",
              "  [2, 11, 0.536392867565155, 0.5083749890327454, 0.23655147850513458],\n",
              "  [2, 12, 0.5361339449882507, 0.5083125233650208, 0.23647505044937134],\n",
              "  [2, 13, 0.5363125205039978, 0.508062481880188, 0.23640353977680206],\n",
              "  [2, 14, 0.5364375114440918, 0.507687509059906, 0.23633666336536407],\n",
              "  [2, 15, 0.536517858505249, 0.507562518119812, 0.23626329004764557],\n",
              "  [2, 16, 0.5366339087486267, 0.5072500109672546, 0.2361851930618286],\n",
              "  [2, 17, 0.5367767810821533, 0.507437527179718, 0.23610979318618774],\n",
              "  [2, 18, 0.5368303656578064, 0.5073124766349792, 0.236028254032135],\n",
              "  [2, 19, 0.5369553565979004, 0.5068749785423279, 0.23594200611114502],\n",
              "  [2, 20, 0.5370446443557739, 0.5070000290870667, 0.23585860431194305],\n",
              "  [2, 21, 0.5372589230537415, 0.5070000290870667, 0.2357587218284607],\n",
              "  [2, 22, 0.5373928546905518, 0.5065624713897705, 0.23564958572387695],\n",
              "  [2, 23, 0.5374017953872681, 0.5068749785423279, 0.2355397641658783],\n",
              "  [2, 24, 0.5375178456306458, 0.5068125128746033, 0.23542435467243195],\n",
              "  [2, 25, 0.5376874804496765, 0.5069375038146973, 0.23529452085494995],\n",
              "  [2, 26, 0.5377321243286133, 0.5072500109672546, 0.23517051339149475],\n",
              "  [2, 27, 0.5379375219345093, 0.5073750019073486, 0.23504014313220978],\n",
              "  [2, 28, 0.5381518006324768, 0.5068749785423279, 0.23491226136684418],\n",
              "  [2, 29, 0.5382678508758545, 0.5073750019073486, 0.23478317260742188],\n",
              "  [2, 30, 0.538428544998169, 0.5070000290870667, 0.23464831709861755],\n",
              "  [2, 31, 0.5384910702705383, 0.5072500109672546, 0.23451614379882812],\n",
              "  [2, 32, 0.5386964082717896, 0.5078750252723694, 0.23438036441802979],\n",
              "  [2, 33, 0.538687527179718, 0.5084999799728394, 0.23423583805561066],\n",
              "  [2, 34, 0.5389196276664734, 0.5086249709129333, 0.23409007489681244],\n",
              "  [2, 35, 0.5390803813934326, 0.5086874961853027, 0.23395603895187378],\n",
              "  [2, 36, 0.539187490940094, 0.5088750123977661, 0.2338337004184723],\n",
              "  [2, 37, 0.5392678380012512, 0.5090000033378601, 0.23371832072734833],\n",
              "  [2, 38, 0.5396606922149658, 0.5096874833106995, 0.23352575302124023],\n",
              "  [2, 39, 0.5398125052452087, 0.5088124871253967, 0.23341035842895508],\n",
              "  [2, 40, 0.5401607155799866, 0.5093125104904175, 0.2332824170589447],\n",
              "  [2, 41, 0.5400357246398926, 0.5095000267028809, 0.23308023810386658],\n",
              "  [2, 42, 0.5406249761581421, 0.5086874961853027, 0.23296597599983215],\n",
              "  [2, 43, 0.5403035879135132, 0.5090625286102295, 0.23279313743114471],\n",
              "  [2, 44, 0.5404642820358276, 0.5096250176429749, 0.23265112936496735],\n",
              "  [2, 45, 0.5408928394317627, 0.5088124871253967, 0.23244524002075195],\n",
              "  [2, 46, 0.5411160588264465, 0.5082499980926514, 0.23230567574501038],\n",
              "  [2, 47, 0.5413749814033508, 0.5091249942779541, 0.23214435577392578],\n",
              "  [2, 48, 0.5415178537368774, 0.5093125104904175, 0.23194606602191925],\n",
              "  [2, 49, 0.541857123374939, 0.5083749890327454, 0.23179730772972107],\n",
              "  [2, 50, 0.5419374704360962, 0.5086874961853027, 0.231613427400589],\n",
              "  [2, 51, 0.5421428680419922, 0.5088750123977661, 0.23147401213645935],\n",
              "  [2, 52, 0.5423303842544556, 0.5087500214576721, 0.2312687337398529],\n",
              "  [2, 53, 0.5425446629524231, 0.5078125, 0.23112910985946655],\n",
              "  [2, 54, 0.5428571701049805, 0.5088750123977661, 0.2309507578611374],\n",
              "  [2, 55, 0.5426250100135803, 0.5087500214576721, 0.23077420890331268],\n",
              "  [2, 56, 0.5428392887115479, 0.5090000033378601, 0.23064234852790833],\n",
              "  [2, 57, 0.5434910655021667, 0.5090000033378601, 0.2304200977087021],\n",
              "  [2, 58, 0.5435178279876709, 0.5092499852180481, 0.230302631855011],\n",
              "  [2, 59, 0.5436428785324097, 0.5092499852180481, 0.23013600707054138],\n",
              "  [2, 60, 0.5439196228981018, 0.5089374780654907, 0.2300349473953247],\n",
              "  [2, 61, 0.5441874861717224, 0.5094375014305115, 0.22987805306911469],\n",
              "  [2, 62, 0.5442321300506592, 0.5088124871253967, 0.22966542840003967],\n",
              "  [2, 63, 0.5441250205039978, 0.5095000267028809, 0.2295200228691101],\n",
              "  [2, 64, 0.5446696281433105, 0.5096874833106995, 0.2294059693813324],\n",
              "  [2, 65, 0.5444375276565552, 0.5090625286102295, 0.22925196588039398],\n",
              "  [2, 66, 0.5450089573860168, 0.5096250176429749, 0.2291136085987091],\n",
              "  [2, 67, 0.5449196696281433, 0.5089374780654907, 0.22896325588226318],\n",
              "  [2, 68, 0.5447232127189636, 0.5094375014305115, 0.22883997857570648],\n",
              "  [2, 69, 0.5451428294181824, 0.5092499852180481, 0.22866399586200714],\n",
              "  [2, 70, 0.5451785922050476, 0.5084999799728394, 0.2285541594028473],\n",
              "  [2, 71, 0.545285701751709, 0.5092499852180481, 0.22842296957969666],\n",
              "  [2, 72, 0.5456071496009827, 0.5088750123977661, 0.22830873727798462],\n",
              "  [2, 73, 0.5460267663002014, 0.5087500214576721, 0.22816404700279236],\n",
              "  [2, 74, 0.545464277267456, 0.5083749890327454, 0.22802194952964783],\n",
              "  [2, 75, 0.5462321639060974, 0.5086249709129333, 0.22790569067001343],\n",
              "  [2, 76, 0.546116054058075, 0.5080000162124634, 0.22776135802268982],\n",
              "  [2, 77, 0.5463839173316956, 0.5080000162124634, 0.22758886218070984],\n",
              "  [2, 78, 0.5467143058776855, 0.507687509059906, 0.22750578820705414],\n",
              "  [2, 79, 0.5470714569091797, 0.5072500109672546, 0.22737984359264374],\n",
              "  [2, 80, 0.5467321276664734, 0.5080000162124634, 0.22728562355041504],\n",
              "  [2, 81, 0.5469821691513062, 0.5085625052452087, 0.22711491584777832],\n",
              "  [2, 82, 0.5472946166992188, 0.5085625052452087, 0.22697164118289948],\n",
              "  [2, 83, 0.5473839044570923, 0.5088750123977661, 0.2268817126750946],\n",
              "  [2, 84, 0.5473214387893677, 0.5086874961853027, 0.22679321467876434],\n",
              "  [2, 85, 0.5476160645484924, 0.5087500214576721, 0.22660194337368011],\n",
              "  [2, 86, 0.5478928685188293, 0.5087500214576721, 0.22649627923965454],\n",
              "  [2, 87, 0.5477410554885864, 0.5080000162124634, 0.22641782462596893],\n",
              "  [2, 88, 0.5481964349746704, 0.5081250071525574, 0.22626766562461853],\n",
              "  [2, 89, 0.5484910607337952, 0.5083749890327454, 0.22608822584152222],\n",
              "  [2, 90, 0.5483839511871338, 0.5084375143051147, 0.22602520883083344],\n",
              "  [2, 91, 0.5485714077949524, 0.5081250071525574, 0.22592969238758087],\n",
              "  [2, 92, 0.5491339564323425, 0.5078750252723694, 0.22578741610050201],\n",
              "  [2, 93, 0.5489821434020996, 0.5086874961853027, 0.22567106783390045],\n",
              "  [2, 94, 0.5491428375244141, 0.5090625286102295, 0.2255389541387558],\n",
              "  [2, 95, 0.549419641494751, 0.5087500214576721, 0.2254798412322998],\n",
              "  [2, 96, 0.5496160984039307, 0.5085625052452087, 0.225362166762352],\n",
              "  [2, 97, 0.5493571162223816, 0.5090000033378601, 0.2251981943845749],\n",
              "  [2, 98, 0.5498660802841187, 0.5095624923706055, 0.2250968962907791],\n",
              "  [2, 99, 0.5497767925262451, 0.5084375143051147, 0.22504666447639465],\n",
              "  [2, 100, 0.5499285459518433, 0.5100625157356262, 0.22496123611927032],\n",
              "  [2, 101, 0.5502767562866211, 0.5090625286102295, 0.2248811423778534],\n",
              "  [2, 102, 0.5503928661346436, 0.5093749761581421, 0.22479070723056793],\n",
              "  [2, 103, 0.550678551197052, 0.5094375014305115, 0.2245958149433136],\n",
              "  [2, 104, 0.5503392815589905, 0.5084999799728394, 0.224531352519989],\n",
              "  [2, 105, 0.5507767796516418, 0.5098749995231628, 0.22438067197799683],\n",
              "  [2, 106, 0.5507678389549255, 0.5094375014305115, 0.2243565320968628],\n",
              "  [2, 107, 0.5512499809265137, 0.5091875195503235, 0.22428004443645477],\n",
              "  [2, 108, 0.5509285926818848, 0.5087500214576721, 0.22410908341407776],\n",
              "  [2, 109, 0.5514285564422607, 0.5095624923706055, 0.2240116149187088],\n",
              "  [2, 110, 0.5520356893539429, 0.5091249942779541, 0.2239101529121399],\n",
              "  [2, 111, 0.5511428713798523, 0.5092499852180481, 0.2238391637802124],\n",
              "  [2, 112, 0.5517857074737549, 0.5080000162124634, 0.22380779683589935],\n",
              "  [2, 113, 0.5522767901420593, 0.508062481880188, 0.223636195063591],\n",
              "  [2, 114, 0.5521249771118164, 0.5088750123977661, 0.22354593873023987],\n",
              "  [2, 115, 0.5519285798072815, 0.5088750123977661, 0.22350847721099854],\n",
              "  [2, 116, 0.5523124933242798, 0.5083749890327454, 0.22341367602348328],\n",
              "  [2, 117, 0.5526339411735535, 0.507687509059906, 0.22335179150104523],\n",
              "  [2, 118, 0.5523481965065002, 0.5074999928474426, 0.22320382297039032],\n",
              "  [2, 119, 0.5523660778999329, 0.5083125233650208, 0.223161980509758],\n",
              "  [2, 120, 0.5526964068412781, 0.5083125233650208, 0.2230594903230667],\n",
              "  [2, 121, 0.5529375076293945, 0.5078125, 0.2229226976633072],\n",
              "  [2, 122, 0.5529642701148987, 0.5083125233650208, 0.22282415628433228],\n",
              "  [2, 123, 0.5530892610549927, 0.5081250071525574, 0.22273555397987366],\n",
              "  [2, 124, 0.5530357360839844, 0.5078125, 0.22268685698509216],\n",
              "  [2, 125, 0.5534999966621399, 0.5089374780654907, 0.22255270183086395],\n",
              "  [2, 126, 0.5536339282989502, 0.5080000162124634, 0.222464457154274],\n",
              "  [2, 127, 0.5535446405410767, 0.5083125233650208, 0.22238335013389587],\n",
              "  [2, 128, 0.5533839464187622, 0.507937490940094, 0.2223048061132431],\n",
              "  [2, 129, 0.5532678365707397, 0.5084375143051147, 0.22225764393806458],\n",
              "  [2, 130, 0.5538482069969177, 0.5073750019073486, 0.2221817970275879],\n",
              "  [2, 131, 0.5535982251167297, 0.507937490940094, 0.2221032828092575],\n",
              "  [2, 132, 0.5538303852081299, 0.507562518119812, 0.22203092277050018],\n",
              "  [2, 133, 0.5540981888771057, 0.508062481880188, 0.2219763696193695],\n",
              "  [2, 134, 0.5540981888771057, 0.5066249966621399, 0.2219724804162979],\n",
              "  [2, 135, 0.5539017915725708, 0.508062481880188, 0.2220609486103058],\n",
              "  [2, 136, 0.5540893077850342, 0.5071250200271606, 0.22195641696453094],\n",
              "  [2, 137, 0.5545714497566223, 0.507437527179718, 0.22169733047485352],\n",
              "  [2, 138, 0.5546964406967163, 0.5078750252723694, 0.2216159701347351],\n",
              "  [2, 139, 0.5542589426040649, 0.5068749785423279, 0.22162696719169617],\n",
              "  [2, 140, 0.5543214082717896, 0.5077499747276306, 0.22160597145557404],\n",
              "  [2, 141, 0.5547232031822205, 0.5077499747276306, 0.22143204510211945],\n",
              "  [2, 142, 0.5545356869697571, 0.5069375038146973, 0.2213730365037918],\n",
              "  [2, 143, 0.5545892715454102, 0.507437527179718, 0.22134684026241302],\n",
              "  [2, 144, 0.5550178289413452, 0.5081250071525574, 0.22121191024780273],\n",
              "  [2, 145, 0.5549196600914001, 0.5070000290870667, 0.2211781144142151],\n",
              "  [2, 146, 0.5549910664558411, 0.5070000290870667, 0.2210395485162735],\n",
              "  [2, 147, 0.5547321438789368, 0.5078125, 0.2209904044866562],\n",
              "  [2, 148, 0.5557143092155457, 0.5070000290870667, 0.2209712117910385],\n",
              "  [2, 149, 0.5551607012748718, 0.5078750252723694, 0.2208801656961441],\n",
              "  [2, 150, 0.5552321672439575, 0.5073750019073486, 0.2208200842142105],\n",
              "  [2, 151, 0.5557767748832703, 0.5071874856948853, 0.22077512741088867],\n",
              "  [2, 152, 0.5555535554885864, 0.5083125233650208, 0.2206982970237732],\n",
              "  [2, 153, 0.5558750033378601, 0.5084999799728394, 0.22061799466609955],\n",
              "  [2, 154, 0.5557500123977661, 0.5071250200271606, 0.2206345796585083],\n",
              "  [2, 155, 0.556071400642395, 0.5082499980926514, 0.2206009179353714],\n",
              "  [2, 156, 0.5555892586708069, 0.5094375014305115, 0.22038495540618896],\n",
              "  [2, 157, 0.5559196472167969, 0.5076249837875366, 0.22034217417240143],\n",
              "  [2, 158, 0.5566874742507935, 0.5086249709129333, 0.22022058069705963],\n",
              "  [2, 159, 0.5559553503990173, 0.5077499747276306, 0.22015731036663055],\n",
              "  [2, 160, 0.5562232136726379, 0.5077499747276306, 0.22018282115459442],\n",
              "  [2, 161, 0.5562410950660706, 0.507937490940094, 0.2200927734375],\n",
              "  [2, 162, 0.5561964511871338, 0.5081250071525574, 0.21999101340770721],\n",
              "  [2, 163, 0.5564464330673218, 0.5081250071525574, 0.2199302315711975],\n",
              "  [2, 164, 0.5563303828239441, 0.5073750019073486, 0.219899982213974],\n",
              "  [2, 165, 0.5566071271896362, 0.5088124871253967, 0.21981951594352722],\n",
              "  [2, 166, 0.5566160678863525, 0.5083125233650208, 0.2197783887386322],\n",
              "  [2, 167, 0.556857168674469, 0.5086249709129333, 0.21963481605052948],\n",
              "  [2, 168, 0.5564464330673218, 0.5092499852180481, 0.2195972353219986],\n",
              "  [2, 169, 0.5565981864929199, 0.5073750019073486, 0.21965403854846954],\n",
              "  [2, 170, 0.5569911003112793, 0.5089374780654907, 0.2195243090391159],\n",
              "  [2, 171, 0.5570268034934998, 0.5092499852180481, 0.2193933129310608],\n",
              "  [2, 172, 0.5571249723434448, 0.5083125233650208, 0.21934111416339874],\n",
              "  [2, 173, 0.5569106936454773, 0.5092499852180481, 0.21926240622997284],\n",
              "  [2, 174, 0.5572589039802551, 0.5086249709129333, 0.21922630071640015],\n",
              "  [2, 175, 0.5567499995231628, 0.5096250176429749, 0.21920481324195862],\n",
              "  [2, 176, 0.5573035478591919, 0.5078125, 0.21925735473632812],\n",
              "  [2, 177, 0.5571964383125305, 0.5090000033378601, 0.2192118912935257],\n",
              "  [2, 178, 0.5572232007980347, 0.5084375143051147, 0.21909716725349426],\n",
              "  [2, 179, 0.5572232007980347, 0.5081250071525574, 0.2189851999282837],\n",
              "  [2, 180, 0.5572410821914673, 0.5091875195503235, 0.2189328372478485],\n",
              "  [2, 181, 0.5576964020729065, 0.508187472820282, 0.21894726157188416],\n",
              "  [2, 182, 0.5570803284645081, 0.5091249942779541, 0.2189272940158844],\n",
              "  [2, 183, 0.5575267672538757, 0.507562518119812, 0.21889673173427582],\n",
              "  [2, 184, 0.5577053427696228, 0.508187472820282, 0.2187163084745407],\n",
              "  [2, 185, 0.5575000047683716, 0.5093749761581421, 0.2186899185180664],\n",
              "  [2, 186, 0.5577499866485596, 0.507562518119812, 0.21881616115570068],\n",
              "  [2, 187, 0.557535707950592, 0.5083749890327454, 0.21875804662704468],\n",
              "  [2, 188, 0.5580178499221802, 0.5090000033378601, 0.2186049520969391],\n",
              "  [2, 189, 0.5577678680419922, 0.5077499747276306, 0.218544140458107],\n",
              "  [2, 190, 0.5577232241630554, 0.5081250071525574, 0.21853555738925934],\n",
              "  [2, 191, 0.5580267906188965, 0.5083749890327454, 0.21848903596401215],\n",
              "  [2, 192, 0.5578571557998657, 0.5093749761581421, 0.21840962767601013],\n",
              "  [2, 193, 0.558241069316864, 0.5087500214576721, 0.2184186726808548],\n",
              "  [2, 194, 0.557991087436676, 0.5088124871253967, 0.21833963692188263],\n",
              "  [2, 195, 0.5582946538925171, 0.5081250071525574, 0.21834437549114227],\n",
              "  [2, 196, 0.5580624938011169, 0.5085625052452087, 0.21822886168956757],\n",
              "  [2, 197, 0.5578482151031494, 0.5087500214576721, 0.21816954016685486],\n",
              "  [2, 198, 0.5584732294082642, 0.5083125233650208, 0.21815051138401031],\n",
              "  [2, 199, 0.5581696629524231, 0.5085625052452087, 0.21816319227218628],\n",
              "  [2, 200, 0.5583928823471069, 0.5091249942779541, 0.21820692718029022],\n",
              "  [3, 1, 0.5572410821914673, 0.5093125104904175, 0.2390744984149933],\n",
              "  [3, 2, 0.5575267672538757, 0.5103124976158142, 0.23126916587352753],\n",
              "  [3, 3, 0.5576696395874023, 0.5113750100135803, 0.23012249171733856],\n",
              "  [3, 4, 0.557660698890686, 0.5105624794960022, 0.229359433054924],\n",
              "  [3, 5, 0.557687520980835, 0.510937511920929, 0.2291102409362793],\n",
              "  [3, 6, 0.5577499866485596, 0.5106250047683716, 0.22900305688381195],\n",
              "  [3, 7, 0.5576339364051819, 0.5107499957084656, 0.2289077192544937],\n",
              "  [3, 8, 0.5580000281333923, 0.510812520980835, 0.22879639267921448],\n",
              "  [3, 9, 0.5582589507102966, 0.5107499957084656, 0.22872257232666016],\n",
              "  [3, 10, 0.5582321286201477, 0.5108749866485596, 0.22866317629814148],\n",
              "  [3, 11, 0.5584375262260437, 0.5103124976158142, 0.22859618067741394],\n",
              "  [3, 12, 0.558491051197052, 0.5101875066757202, 0.22853180766105652],\n",
              "  [3, 13, 0.5583571195602417, 0.5100625157356262, 0.22845721244812012],\n",
              "  [3, 14, 0.5582946538925171, 0.5101875066757202, 0.22838923335075378],\n",
              "  [3, 15, 0.5581428408622742, 0.5103124976158142, 0.2283250242471695],\n",
              "  [3, 16, 0.5582321286201477, 0.5097500085830688, 0.22826984524726868],\n",
              "  [3, 17, 0.5583214163780212, 0.5098749995231628, 0.228200763463974],\n",
              "  [3, 18, 0.5585356950759888, 0.5098749995231628, 0.22811724245548248],\n",
              "  [3, 19, 0.5583928823471069, 0.5101249814033508, 0.22804446518421173],\n",
              "  [3, 20, 0.558491051197052, 0.5100625157356262, 0.2279578149318695],\n",
              "  [3, 21, 0.5586696267127991, 0.5098124742507935, 0.2278645783662796],\n",
              "  [3, 22, 0.5588839054107666, 0.5101249814033508, 0.22776971757411957],\n",
              "  [3, 23, 0.5587410926818848, 0.5103124976158142, 0.22767069935798645],\n",
              "  [3, 24, 0.5588392615318298, 0.5100625157356262, 0.22757776081562042],\n",
              "  [3, 25, 0.5591428279876709, 0.5105000138282776, 0.22746528685092926],\n",
              "  [3, 26, 0.5592767596244812, 0.5103124976158142, 0.227362722158432],\n",
              "  [3, 27, 0.559580385684967, 0.5109999775886536, 0.22726139426231384],\n",
              "  [3, 28, 0.5596874952316284, 0.5106250047683716, 0.22714553773403168],\n",
              "  [3, 29, 0.5597678422927856, 0.5103124976158142, 0.22703917324543],\n",
              "  [3, 30, 0.5600000023841858, 0.5104374885559082, 0.22691898047924042],\n",
              "  [3, 31, 0.5600982308387756, 0.5099375247955322, 0.22681663930416107],\n",
              "  [3, 32, 0.5603125095367432, 0.5101249814033508, 0.22670046985149384],\n",
              "  [3, 33, 0.5606071352958679, 0.5103124976158142, 0.22659114003181458],\n",
              "  [3, 34, 0.5607589483261108, 0.5107499957084656, 0.2264677733182907],\n",
              "  [3, 35, 0.5607143044471741, 0.5102499723434448, 0.2263547033071518],\n",
              "  [3, 36, 0.5609375238418579, 0.510937511920929, 0.22625058889389038],\n",
              "  [3, 37, 0.5609017610549927, 0.5109999775886536, 0.226129949092865],\n",
              "  [3, 38, 0.5612499713897705, 0.5105624794960022, 0.2260071188211441],\n",
              "  [3, 39, 0.561366081237793, 0.510937511920929, 0.22591669857501984],\n",
              "  [3, 40, 0.5613481998443604, 0.5113124847412109, 0.22583195567131042],\n",
              "  [3, 41, 0.5618214011192322, 0.5108749866485596, 0.2257213145494461],\n",
              "  [3, 42, 0.5616071224212646, 0.5101875066757202, 0.22565661370754242],\n",
              "  [3, 43, 0.5615267753601074, 0.511062502861023, 0.22546079754829407],\n",
              "  [3, 44, 0.5618482232093811, 0.5111874938011169, 0.22543774545192719],\n",
              "  [3, 45, 0.5620267987251282, 0.5112500190734863, 0.22529339790344238],\n",
              "  [3, 46, 0.5621785521507263, 0.5111250281333923, 0.22517141699790955],\n",
              "  [3, 47, 0.562375009059906, 0.5103750228881836, 0.22503815591335297],\n",
              "  [3, 48, 0.5624553561210632, 0.511062502861023, 0.22495345771312714],\n",
              "  [3, 49, 0.5627053380012512, 0.5106874704360962, 0.22481495141983032],\n",
              "  [3, 50, 0.5626875162124634, 0.5111874938011169, 0.22474654018878937],\n",
              "  [3, 51, 0.5628035664558411, 0.5111250281333923, 0.22465188801288605],\n",
              "  [3, 52, 0.5630267858505249, 0.510812520980835, 0.22454728186130524],\n",
              "  [3, 53, 0.563080370426178, 0.5106250047683716, 0.22442756593227386],\n",
              "  [3, 54, 0.563080370426178, 0.5105000138282776, 0.2243407517671585],\n",
              "  [3, 55, 0.5634642839431763, 0.5111250281333923, 0.2242487221956253],\n",
              "  [3, 56, 0.5636249780654907, 0.5103124976158142, 0.22413897514343262],\n",
              "  [3, 57, 0.5634999871253967, 0.5102499723434448, 0.22399847209453583],\n",
              "  [3, 58, 0.5633660554885864, 0.5106874704360962, 0.22392556071281433],\n",
              "  [3, 59, 0.5636160969734192, 0.5107499957084656, 0.22383499145507812],\n",
              "  [3, 60, 0.5641607046127319, 0.5097500085830688, 0.2237338125705719],\n",
              "  [3, 61, 0.5638035535812378, 0.5103124976158142, 0.22365981340408325],\n",
              "  [3, 62, 0.5639731884002686, 0.5098124742507935, 0.22356058657169342],\n",
              "  [3, 63, 0.5642678737640381, 0.5101875066757202, 0.2234250158071518],\n",
              "  [3, 64, 0.5641696453094482, 0.5107499957084656, 0.2233620584011078],\n",
              "  [3, 65, 0.5641785860061646, 0.5097500085830688, 0.2233816236257553],\n",
              "  [3, 66, 0.5644999742507935, 0.5097500085830688, 0.22318342328071594],\n",
              "  [3, 67, 0.5644285678863525, 0.5111250281333923, 0.2232763022184372],\n",
              "  [3, 68, 0.5642321705818176, 0.510812520980835, 0.22320793569087982],\n",
              "  [3, 69, 0.5645357370376587, 0.5097500085830688, 0.22303199768066406],\n",
              "  [3, 70, 0.5645982027053833, 0.5102499723434448, 0.22300229966640472],\n",
              "  [3, 71, 0.5643839240074158, 0.5114374756813049, 0.22285553812980652],\n",
              "  [3, 72, 0.5642589330673218, 0.511062502861023, 0.22284050285816193],\n",
              "  [3, 73, 0.5647857189178467, 0.5091875195503235, 0.2227005809545517],\n",
              "  [3, 74, 0.5651249885559082, 0.5095624923706055, 0.22265008091926575],\n",
              "  [3, 75, 0.5643928647041321, 0.5101875066757202, 0.22255541384220123],\n",
              "  [3, 76, 0.5649464130401611, 0.510812520980835, 0.22249038517475128],\n",
              "  [3, 77, 0.5654821395874023, 0.5106250047683716, 0.22239191830158234],\n",
              "  [3, 78, 0.5654285550117493, 0.5093749761581421, 0.22234885394573212],\n",
              "  [3, 79, 0.5650625228881836, 0.5099999904632568, 0.22219747304916382],\n",
              "  [3, 80, 0.5651339292526245, 0.5096874833106995, 0.22214163839817047],\n",
              "  [3, 81, 0.5657589435577393, 0.5100625157356262, 0.22208106517791748],\n",
              "  [3, 82, 0.5657053589820862, 0.5101249814033508, 0.22202801704406738],\n",
              "  [3, 83, 0.5654910802841187, 0.5107499957084656, 0.22192388772964478],\n",
              "  [3, 84, 0.5657767653465271, 0.5101875066757202, 0.22182488441467285],\n",
              "  [3, 85, 0.5659732222557068, 0.5102499723434448, 0.22172081470489502],\n",
              "  [3, 86, 0.5659106969833374, 0.5106874704360962, 0.2216433733701706],\n",
              "  [3, 87, 0.5656696557998657, 0.5103124976158142, 0.22157059609889984],\n",
              "  [3, 88, 0.5660178661346436, 0.5103124976158142, 0.2215147316455841],\n",
              "  [3, 89, 0.5662857294082642, 0.5100625157356262, 0.22145256400108337],\n",
              "  [3, 90, 0.5663303732872009, 0.5101875066757202, 0.2214239090681076],\n",
              "  [3, 91, 0.5657321214675903, 0.5103750228881836, 0.22138749063014984],\n",
              "  [3, 92, 0.5660982131958008, 0.5098749995231628, 0.22135618329048157],\n",
              "  [3, 93, 0.5663571357727051, 0.5102499723434448, 0.22119221091270447],\n",
              "  [3, 94, 0.5662500262260437, 0.5104374885559082, 0.2211090624332428],\n",
              "  [3, 95, 0.566428542137146, 0.5103750228881836, 0.22105559706687927],\n",
              "  [3, 96, 0.5662767887115479, 0.5109999775886536, 0.22108572721481323],\n",
              "  [3, 97, 0.5661160945892334, 0.5104374885559082, 0.22111408412456512],\n",
              "  [3, 98, 0.566383957862854, 0.5108749866485596, 0.22091057896614075],\n",
              "  [3, 99, 0.5665624737739563, 0.5111250281333923, 0.22084134817123413],\n",
              "  [3, 100, 0.5667142868041992, 0.5109999775886536, 0.22086472809314728],\n",
              "  [3, 101, 0.5663928389549255, 0.511062502861023, 0.2207648903131485],\n",
              "  [3, 102, 0.5664910674095154, 0.5117499828338623, 0.22077129781246185],\n",
              "  [3, 103, 0.5672231912612915, 0.5111250281333923, 0.22059398889541626],\n",
              "  [3, 104, 0.5670357346534729, 0.512374997138977, 0.22060324251651764],\n",
              "  [3, 105, 0.5670446157455444, 0.5113124847412109, 0.2207297384738922],\n",
              "  [3, 106, 0.5672053694725037, 0.5108749866485596, 0.2204190194606781],\n",
              "  [3, 107, 0.5670267939567566, 0.5128750205039978, 0.22060613334178925],\n",
              "  [3, 108, 0.5669553279876709, 0.5116249918937683, 0.22064784169197083],\n",
              "  [3, 109, 0.5675535798072815, 0.5112500190734863, 0.2204349935054779],\n",
              "  [3, 110, 0.5673571228981018, 0.5121874809265137, 0.2205168455839157],\n",
              "  [3, 111, 0.5673214197158813, 0.5113124847412109, 0.22049547731876373],\n",
              "  [3, 112, 0.5677232146263123, 0.5120000243186951, 0.22027741372585297],\n",
              "  [3, 113, 0.567642867565155, 0.5131875276565552, 0.22028976678848267],\n",
              "  [3, 114, 0.5674910545349121, 0.5126875042915344, 0.22030574083328247],\n",
              "  [3, 115, 0.5675089359283447, 0.5128124952316284, 0.22008131444454193],\n",
              "  [3, 116, 0.5676160454750061, 0.5133749842643738, 0.22021526098251343],\n",
              "  [3, 117, 0.5680535435676575, 0.5122500061988831, 0.21998463571071625],\n",
              "  [3, 118, 0.5674285888671875, 0.5122500061988831, 0.22001075744628906],\n",
              "  [3, 119, 0.5675267577171326, 0.5131250023841858, 0.21984486281871796],\n",
              "  [3, 120, 0.5680535435676575, 0.5128124952316284, 0.21981731057167053],\n",
              "  [3, 121, 0.5682767629623413, 0.5126875042915344, 0.21973462402820587],\n",
              "  [3, 122, 0.5677589178085327, 0.5131250023841858, 0.2196642905473709],\n",
              "  [3, 123, 0.5678303837776184, 0.5130000114440918, 0.21967503428459167],\n",
              "  [3, 124, 0.568348228931427, 0.5119374990463257, 0.2195124626159668],\n",
              "  [3, 125, 0.568223237991333, 0.5132499933242798, 0.2195807248353958],\n",
              "  [3, 126, 0.5683035850524902, 0.5128124952316284, 0.21943791210651398],\n",
              "  [3, 127, 0.5683035850524902, 0.5127500295639038, 0.21938619017601013],\n",
              "  [3, 128, 0.5686428546905518, 0.5127500295639038, 0.2193574458360672],\n",
              "  [3, 129, 0.5685356855392456, 0.5130000114440918, 0.21926656365394592],\n",
              "  [3, 130, 0.5682678818702698, 0.5133749842643738, 0.2192436307668686],\n",
              "  [3, 131, 0.5683749914169312, 0.5131875276565552, 0.2191658616065979],\n",
              "  [3, 132, 0.5684821605682373, 0.5136874914169312, 0.21915334463119507],\n",
              "  [3, 133, 0.5687410831451416, 0.5130624771118164, 0.21904528141021729],\n",
              "  [3, 134, 0.5688214302062988, 0.5133125185966492, 0.2190513014793396],\n",
              "  [3, 135, 0.5689464211463928, 0.5132499933242798, 0.21894660592079163],\n",
              "  [3, 136, 0.568928599357605, 0.5135625004768372, 0.2189064770936966],\n",
              "  [3, 137, 0.5687589049339294, 0.5133749842643738, 0.2188715636730194],\n",
              "  [3, 138, 0.5689910650253296, 0.5138750076293945, 0.21880733966827393],\n",
              "  [3, 139, 0.5693214535713196, 0.5128750205039978, 0.21879027783870697],\n",
              "  [3, 140, 0.568928599357605, 0.5130624771118164, 0.21873457729816437],\n",
              "  [3, 141, 0.5691606998443604, 0.5135625004768372, 0.21873410046100616],\n",
              "  [3, 142, 0.5689732432365417, 0.5131250023841858, 0.2187226265668869],\n",
              "  [3, 143, 0.5694553852081299, 0.5130000114440918, 0.21886079013347626],\n",
              "  [3, 144, 0.5692142844200134, 0.5139999985694885, 0.21866096556186676],\n",
              "  [3, 145, 0.5691339373588562, 0.5134999752044678, 0.2185641974210739],\n",
              "  [3, 146, 0.5693928599357605, 0.5132499933242798, 0.21852430701255798],\n",
              "  [3, 147, 0.5691964030265808, 0.5134375095367432, 0.21853764355182648],\n",
              "  [3, 148, 0.5693660974502563, 0.5133125185966492, 0.21867991983890533],\n",
              "  [3, 149, 0.5695089101791382, 0.5136874914169312, 0.2184402197599411],\n",
              "  [3, 150, 0.5694375038146973, 0.5141875147819519, 0.21835727989673615],\n",
              "  [3, 151, 0.5696339011192322, 0.5134375095367432, 0.21849071979522705],\n",
              "  [3, 152, 0.5691606998443604, 0.5140625238418579, 0.21839286386966705],\n",
              "  [3, 153, 0.5693660974502563, 0.5139374732971191, 0.21841663122177124],\n",
              "  [3, 154, 0.5695714354515076, 0.5143125057220459, 0.21833914518356323],\n",
              "  [3, 155, 0.5695803761482239, 0.5136250257492065, 0.21834413707256317],\n",
              "  [3, 156, 0.569678544998169, 0.5134999752044678, 0.21840918064117432],\n",
              "  [3, 157, 0.5700535774230957, 0.5145624876022339, 0.2182481735944748],\n",
              "  [3, 158, 0.5695803761482239, 0.5137500166893005, 0.21822862327098846],\n",
              "  [3, 159, 0.5698482394218445, 0.5136874914169312, 0.2184513509273529],\n",
              "  [3, 160, 0.5696428418159485, 0.5133125185966492, 0.21805571019649506],\n",
              "  [3, 161, 0.5699017643928528, 0.5140625238418579, 0.21829329431056976],\n",
              "  [3, 162, 0.569758951663971, 0.5141875147819519, 0.21824544668197632],\n",
              "  [3, 163, 0.5695178508758545, 0.5137500166893005, 0.21810884773731232],\n",
              "  [3, 164, 0.5695178508758545, 0.5136250257492065, 0.21804216504096985],\n",
              "  [3, 165, 0.5695267915725708, 0.5137500166893005, 0.2179425209760666],\n",
              "  [3, 166, 0.5697500109672546, 0.5137500166893005, 0.2180136740207672],\n",
              "  [3, 167, 0.5695624947547913, 0.5143125057220459, 0.21782271564006805],\n",
              "  [3, 168, 0.5704643130302429, 0.5139374732971191, 0.2178448587656021],\n",
              "  [3, 169, 0.5702232122421265, 0.5134999752044678, 0.21770484745502472],\n",
              "  [3, 170, 0.5700803399085999, 0.5133125185966492, 0.21766693890094757],\n",
              "  [3, 171, 0.5700982213020325, 0.5134999752044678, 0.21759237349033356],\n",
              "  [3, 172, 0.5702232122421265, 0.5131875276565552, 0.21750760078430176],\n",
              "  [3, 173, 0.5704017877578735, 0.5136874914169312, 0.21749462187290192],\n",
              "  [3, 174, 0.5708035826683044, 0.5139999985694885, 0.21737833321094513],\n",
              "  [3, 175, 0.5706428289413452, 0.5144374966621399, 0.21740883588790894],\n",
              "  [3, 176, 0.5706071257591248, 0.5142499804496765, 0.2173290103673935],\n",
              "  [3, 177, 0.5706607103347778, 0.5135625004768372, 0.217262402176857],\n",
              "  [3, 178, 0.5707589387893677, 0.5139374732971191, 0.2172805368900299],\n",
              "  [3, 179, 0.5714017748832703, 0.5145624876022339, 0.2171989232301712],\n",
              "  [3, 180, 0.5707142949104309, 0.5133125185966492, 0.2171890288591385],\n",
              "  [3, 181, 0.5709018111228943, 0.5141249895095825, 0.21708884835243225],\n",
              "  [3, 182, 0.5714821219444275, 0.5148749947547913, 0.21698808670043945],\n",
              "  [3, 183, 0.5714285969734192, 0.5141249895095825, 0.21699807047843933],\n",
              "  [3, 184, 0.5717321634292603, 0.5145000219345093, 0.21694253385066986],\n",
              "  [3, 185, 0.571321427822113, 0.5140625238418579, 0.21692028641700745],\n",
              "  [3, 186, 0.5717053413391113, 0.5143749713897705, 0.2168276160955429],\n",
              "  [3, 187, 0.5717053413391113, 0.5141875147819519, 0.2167416363954544],\n",
              "  [3, 188, 0.5716339349746704, 0.5133749842643738, 0.21675164997577667],\n",
              "  [3, 189, 0.5719732046127319, 0.5147500038146973, 0.21673418581485748],\n",
              "  [3, 190, 0.5715000033378601, 0.5142499804496765, 0.21670399606227875],\n",
              "  [3, 191, 0.5719107389450073, 0.5134999752044678, 0.21657942235469818],\n",
              "  [3, 192, 0.5720535516738892, 0.5146250128746033, 0.2165699005126953],\n",
              "  [3, 193, 0.5717142820358276, 0.5130000114440918, 0.21661102771759033],\n",
              "  [3, 194, 0.5716696381568909, 0.5136250257492065, 0.21659862995147705],\n",
              "  [3, 195, 0.5714910626411438, 0.5138750076293945, 0.2165040224790573],\n",
              "  [3, 196, 0.5722678303718567, 0.5135625004768372, 0.21645943820476532],\n",
              "  [3, 197, 0.5719821453094482, 0.5141875147819519, 0.21661579608917236],\n",
              "  [3, 198, 0.5716160535812378, 0.5130624771118164, 0.21634580194950104],\n",
              "  [3, 199, 0.5723749995231628, 0.5133125185966492, 0.21634544432163239],\n",
              "  [3, 200, 0.5722500085830688, 0.5138124823570251, 0.21628542244434357],\n",
              "  [4, 1, 0.5751428604125977, 0.5148749947547913, 0.2319977879524231],\n",
              "  [4, 2, 0.5754374861717224, 0.5146874785423279, 0.22841762006282806],\n",
              "  [4, 3, 0.5760714411735535, 0.515250027179718, 0.2254173904657364],\n",
              "  [4, 4, 0.5760625004768372, 0.5149999856948853, 0.2247510701417923],\n",
              "  [4, 5, 0.5761517882347107, 0.5145000219345093, 0.2243821918964386],\n",
              "  [4, 6, 0.575830340385437, 0.5147500038146973, 0.22395648062229156],\n",
              "  [4, 7, 0.5757321715354919, 0.515250027179718, 0.22368061542510986],\n",
              "  [4, 8, 0.5755892992019653, 0.515500009059906, 0.2235344648361206],\n",
              "  [4, 9, 0.5756696462631226, 0.515749990940094, 0.22345055639743805],\n",
              "  [4, 10, 0.5755535960197449, 0.5160624980926514, 0.223399817943573],\n",
              "  [4, 11, 0.5757142901420593, 0.5161250233650208, 0.22335341572761536],\n",
              "  [4, 12, 0.5760982036590576, 0.5166875123977661, 0.22329765558242798],\n",
              "  [4, 13, 0.5764374732971191, 0.5169374942779541, 0.22322504222393036],\n",
              "  [4, 14, 0.576660692691803, 0.5169374942779541, 0.2231415957212448],\n",
              "  [4, 15, 0.5768035650253296, 0.5165625214576721, 0.22306151688098907],\n",
              "  [4, 16, 0.5769821405410767, 0.5164999961853027, 0.2229856252670288],\n",
              "  [4, 17, 0.5770893096923828, 0.5166875123977661, 0.2229083627462387],\n",
              "  [4, 18, 0.5769464373588562, 0.515874981880188, 0.22283102571964264],\n",
              "  [4, 19, 0.576964259147644, 0.515749990940094, 0.22274991869926453],\n",
              "  [4, 20, 0.576741099357605, 0.515874981880188, 0.2226596474647522],\n",
              "  [4, 21, 0.576866090297699, 0.5155624747276306, 0.22256764769554138],\n",
              "  [4, 22, 0.5770624876022339, 0.5151249766349792, 0.22247642278671265],\n",
              "  [4, 23, 0.5771785974502563, 0.515375018119812, 0.22237172722816467],\n",
              "  [4, 24, 0.5774821639060974, 0.5156875252723694, 0.22226102650165558],\n",
              "  [4, 25, 0.5774106979370117, 0.515999972820282, 0.22214174270629883],\n",
              "  [4, 26, 0.5775089263916016, 0.5164374709129333, 0.2220144420862198],\n",
              "  [4, 27, 0.5776964426040649, 0.515999972820282, 0.22188636660575867],\n",
              "  [4, 28, 0.5778393149375916, 0.515874981880188, 0.2217499315738678],\n",
              "  [4, 29, 0.5777767896652222, 0.5158125162124634, 0.22160553932189941],\n",
              "  [4, 30, 0.5779821276664734, 0.515874981880188, 0.22146379947662354],\n",
              "  [4, 31, 0.5778838992118835, 0.5161874890327454, 0.22131986916065216],\n",
              "  [4, 32, 0.5778838992118835, 0.5159375071525574, 0.2211703211069107],\n",
              "  [4, 33, 0.5781160593032837, 0.5151875019073486, 0.2210109382867813],\n",
              "  [4, 34, 0.5785446166992188, 0.5154374837875366, 0.2208566665649414],\n",
              "  [4, 35, 0.5787678360939026, 0.5156875252723694, 0.22068826854228973],\n",
              "  [4, 36, 0.5788482427597046, 0.515500009059906, 0.2205207198858261],\n",
              "  [4, 37, 0.5789732336997986, 0.515999972820282, 0.22033895552158356],\n",
              "  [4, 38, 0.5791875123977661, 0.5163750052452087, 0.2201625555753708],\n",
              "  [4, 39, 0.5791964530944824, 0.5161250233650208, 0.21997959911823273],\n",
              "  [4, 40, 0.5793214440345764, 0.515749990940094, 0.21979761123657227],\n",
              "  [4, 41, 0.5795714259147644, 0.5160624980926514, 0.21961240470409393],\n",
              "  [4, 42, 0.5796428322792053, 0.515625, 0.2194223403930664],\n",
              "  [4, 43, 0.580214262008667, 0.5159375071525574, 0.21923471987247467],\n",
              "  [4, 44, 0.5803839564323425, 0.515625, 0.21905405819416046],\n",
              "  [4, 45, 0.5808928608894348, 0.515500009059906, 0.21887750923633575],\n",
              "  [4, 46, 0.5809821486473083, 0.5155624747276306, 0.21868132054805756],\n",
              "  [4, 47, 0.5814464092254639, 0.5154374837875366, 0.21845278143882751],\n",
              "  [4, 48, 0.5814196467399597, 0.5156875252723694, 0.2182319015264511],\n",
              "  [4, 49, 0.5814821720123291, 0.515375018119812, 0.21804532408714294],\n",
              "  [4, 50, 0.581803560256958, 0.5154374837875366, 0.21786044538021088],\n",
              "  [4, 51, 0.5819553732872009, 0.515500009059906, 0.21764415502548218],\n",
              "  [4, 52, 0.5820624828338623, 0.5156875252723694, 0.2174508273601532],\n",
              "  [4, 53, 0.5823571681976318, 0.5151249766349792, 0.21724726259708405],\n",
              "  [4, 54, 0.5821964144706726, 0.5151249766349792, 0.21707658469676971],\n",
              "  [4, 55, 0.5827142596244812, 0.515250027179718, 0.21688154339790344],\n",
              "  [4, 56, 0.5831606984138489, 0.5154374837875366, 0.2166905552148819],\n",
              "  [4, 57, 0.5833035707473755, 0.5148749947547913, 0.21650047600269318],\n",
              "  [4, 58, 0.5833303332328796, 0.5151249766349792, 0.21630510687828064],\n",
              "  [4, 59, 0.5834285616874695, 0.5151875019073486, 0.21611614525318146],\n",
              "  [4, 60, 0.5839464068412781, 0.515500009059906, 0.21598152816295624],\n",
              "  [4, 61, 0.5840089321136475, 0.5156875252723694, 0.21580246090888977],\n",
              "  [4, 62, 0.5847589373588562, 0.5159375071525574, 0.21559736132621765],\n",
              "  [4, 63, 0.584776759147644, 0.515500009059906, 0.21540214121341705],\n",
              "  [4, 64, 0.584803581237793, 0.515375018119812, 0.215214803814888],\n",
              "  [4, 65, 0.5852946639060974, 0.5160624980926514, 0.215065598487854],\n",
              "  [4, 66, 0.5855982303619385, 0.5161250233650208, 0.21488144993782043],\n",
              "  [4, 67, 0.585562527179718, 0.515375018119812, 0.21471627056598663],\n",
              "  [4, 68, 0.585812509059906, 0.5159375071525574, 0.21453030407428741],\n",
              "  [4, 69, 0.5858749747276306, 0.5156875252723694, 0.2144085168838501],\n",
              "  [4, 70, 0.5858214497566223, 0.5155624747276306, 0.21422681212425232],\n",
              "  [4, 71, 0.5859821438789368, 0.5154374837875366, 0.21410848200321198],\n",
              "  [4, 72, 0.5863303542137146, 0.5156875252723694, 0.21395327150821686],\n",
              "  [4, 73, 0.5866071581840515, 0.5158125162124634, 0.2137656956911087],\n",
              "  [4, 74, 0.5869910717010498, 0.5154374837875366, 0.21357634663581848],\n",
              "  [4, 75, 0.5866071581840515, 0.5159375071525574, 0.21342933177947998],\n",
              "  [4, 76, 0.587071418762207, 0.5161874890327454, 0.21328341960906982],\n",
              "  [4, 77, 0.5873125195503235, 0.5156875252723694, 0.21314473450183868],\n",
              "  [4, 78, 0.5872767567634583, 0.5158125162124634, 0.21298746764659882],\n",
              "  [4, 79, 0.5874910950660706, 0.515874981880188, 0.21284835040569305],\n",
              "  [4, 80, 0.5874196290969849, 0.5159375071525574, 0.21265572309494019],\n",
              "  [4, 81, 0.5874732136726379, 0.5158125162124634, 0.21253494918346405],\n",
              "  [4, 82, 0.587401807308197, 0.515500009059906, 0.2123817801475525],\n",
              "  [4, 83, 0.5876071453094482, 0.5166875123977661, 0.2123289406299591],\n",
              "  [4, 84, 0.5875446200370789, 0.5164374709129333, 0.2121705859899521],\n",
              "  [4, 85, 0.5879374742507935, 0.5163124799728394, 0.21200473606586456],\n",
              "  [4, 86, 0.5879999995231628, 0.5164999961853027, 0.21189431846141815],\n",
              "  [4, 87, 0.5880178809165955, 0.5168750286102295, 0.21172267198562622],\n",
              "  [4, 88, 0.5880892872810364, 0.5164374709129333, 0.21160365641117096],\n",
              "  [4, 89, 0.5883839130401611, 0.5164999961853027, 0.21146349608898163],\n",
              "  [4, 90, 0.5884017944335938, 0.5167499780654907, 0.21136341989040375],\n",
              "  [4, 91, 0.5886964201927185, 0.5163124799728394, 0.21126237511634827],\n",
              "  [4, 92, 0.5885000228881836, 0.5155624747276306, 0.21118272840976715],\n",
              "  [4, 93, 0.5891071557998657, 0.515999972820282, 0.2110672891139984],\n",
              "  [4, 94, 0.5889821648597717, 0.5154374837875366, 0.21098314225673676],\n",
              "  [4, 95, 0.589062511920929, 0.5161874890327454, 0.2108788639307022],\n",
              "  [4, 96, 0.589241087436676, 0.5164374709129333, 0.21075613796710968],\n",
              "  [4, 97, 0.5896875262260437, 0.5166249871253967, 0.21062614023685455],\n",
              "  [4, 98, 0.5897142887115479, 0.515874981880188, 0.21049372851848602],\n",
              "  [4, 99, 0.5895446538925171, 0.5155624747276306, 0.21042506396770477],\n",
              "  [4, 100, 0.5900803804397583, 0.5165625214576721, 0.21033677458763123],\n",
              "  [4, 101, 0.5898571610450745, 0.5154374837875366, 0.2101788967847824],\n",
              "  [4, 102, 0.5902946591377258, 0.5158125162124634, 0.2101047784090042],\n",
              "  [4, 103, 0.5897946357727051, 0.5151875019073486, 0.20995458960533142],\n",
              "  [4, 104, 0.5902410745620728, 0.5160624980926514, 0.20985984802246094],\n",
              "  [4, 105, 0.590446412563324, 0.5154374837875366, 0.2097327560186386],\n",
              "  [4, 106, 0.5905267596244812, 0.5151875019073486, 0.2096148580312729],\n",
              "  [4, 107, 0.5904732346534729, 0.5151875019073486, 0.20952095091342926],\n",
              "  [4, 108, 0.5909374952316284, 0.5156875252723694, 0.2094184160232544],\n",
              "  [4, 109, 0.5908214449882507, 0.5161874890327454, 0.2092980593442917],\n",
              "  [4, 110, 0.5908214449882507, 0.5156875252723694, 0.20925353467464447],\n",
              "  [4, 111, 0.5912232398986816, 0.5158125162124634, 0.20915117859840393],\n",
              "  [4, 112, 0.5910714268684387, 0.5158125162124634, 0.20908159017562866],\n",
              "  [4, 113, 0.5912946462631226, 0.5168750286102295, 0.2090388387441635],\n",
              "  [4, 114, 0.5911250114440918, 0.515749990940094, 0.2089749276638031],\n",
              "  [4, 115, 0.5915892720222473, 0.515375018119812, 0.20891833305358887],\n",
              "  [4, 116, 0.5914732217788696, 0.5156875252723694, 0.2087264358997345],\n",
              "  [4, 117, 0.5913839340209961, 0.5161874890327454, 0.2085849642753601],\n",
              "  [4, 118, 0.5921696424484253, 0.5158125162124634, 0.20856884121894836],\n",
              "  [4, 119, 0.5916696190834045, 0.515749990940094, 0.20850563049316406],\n",
              "  [4, 120, 0.5919196605682373, 0.515749990940094, 0.20839940011501312],\n",
              "  [4, 121, 0.5922499895095825, 0.515250027179718, 0.2082911729812622],\n",
              "  [4, 122, 0.5920714139938354, 0.5160624980926514, 0.20818732678890228],\n",
              "  [4, 123, 0.5922499895095825, 0.5161874890327454, 0.20808297395706177],\n",
              "  [4, 124, 0.5922053456306458, 0.5156875252723694, 0.20809008181095123],\n",
              "  [4, 125, 0.5924107432365417, 0.5161874890327454, 0.20791491866111755],\n",
              "  [4, 126, 0.5928928852081299, 0.515874981880188, 0.2078467458486557],\n",
              "  [4, 127, 0.59246426820755, 0.5164999961853027, 0.2077590376138687],\n",
              "  [4, 128, 0.5924285650253296, 0.5158125162124634, 0.2076900750398636],\n",
              "  [4, 129, 0.5930267572402954, 0.515999972820282, 0.20758649706840515],\n",
              "  [4, 130, 0.5926428437232971, 0.5166249871253967, 0.20751336216926575],\n",
              "  [4, 131, 0.5929999947547913, 0.5162500143051147, 0.20743176341056824],\n",
              "  [4, 132, 0.5935893058776855, 0.515874981880188, 0.2073451280593872],\n",
              "  [4, 133, 0.5929821133613586, 0.5165625214576721, 0.207288458943367],\n",
              "  [4, 134, 0.5932410955429077, 0.5168125033378601, 0.20724745094776154],\n",
              "  [4, 135, 0.5935714244842529, 0.5163750052452087, 0.20719365775585175],\n",
              "  [4, 136, 0.593874990940094, 0.5164999961853027, 0.20710772275924683],\n",
              "  [4, 137, 0.5935446619987488, 0.5171250104904175, 0.20701928436756134],\n",
              "  [4, 138, 0.5934731960296631, 0.5164999961853027, 0.20690220594406128],\n",
              "  [4, 139, 0.593874990940094, 0.5164374709129333, 0.20678205788135529],\n",
              "  [4, 140, 0.5935981869697571, 0.5171250104904175, 0.2067493498325348],\n",
              "  [4, 141, 0.5936607122421265, 0.5166875123977661, 0.20666030049324036],\n",
              "  [4, 142, 0.5936964154243469, 0.5168750286102295, 0.20661915838718414],\n",
              "  [4, 143, 0.5939464569091797, 0.5168125033378601, 0.20649567246437073],\n",
              "  [4, 144, 0.5938482284545898, 0.5167499780654907, 0.20645448565483093],\n",
              "  [4, 145, 0.5939018130302429, 0.5168750286102295, 0.2063949555158615],\n",
              "  [4, 146, 0.5941160917282104, 0.5171250104904175, 0.20628634095191956],\n",
              "  [4, 147, 0.5938482284545898, 0.5166249871253967, 0.20624512434005737],\n",
              "  [4, 148, 0.5940803289413452, 0.5174375176429749, 0.20620687305927277],\n",
              "  [4, 149, 0.5944732427597046, 0.5173125267028809, 0.2061052769422531],\n",
              "  [4, 150, 0.5942321419715881, 0.5173125267028809, 0.2060643434524536],\n",
              "  [4, 151, 0.5941160917282104, 0.5170624852180481, 0.20601099729537964],\n",
              "  [4, 152, 0.594303548336029, 0.5180624723434448, 0.2059570848941803],\n",
              "  [4, 153, 0.5946071147918701, 0.5170000195503235, 0.20593467354774475],\n",
              "  [4, 154, 0.594633936882019, 0.5171874761581421, 0.20584911108016968],\n",
              "  [4, 155, 0.5941785573959351, 0.5171250104904175, 0.20582324266433716],\n",
              "  [4, 156, 0.5944732427597046, 0.5170624852180481, 0.20573081076145172],\n",
              "  [4, 157, 0.5949553847312927, 0.5183125138282776, 0.20562773942947388],\n",
              "  [4, 158, 0.594508945941925, 0.5175625085830688, 0.20559535920619965],\n",
              "  [4, 159, 0.5952767729759216, 0.5174999833106995, 0.20554806292057037],\n",
              "  [4, 160, 0.5950267910957336, 0.5174999833106995, 0.20548120141029358],\n",
              "  [4, 161, 0.59470534324646, 0.5174999833106995, 0.2054665982723236],\n",
              "  [4, 162, 0.5952589511871338, 0.5174375176429749, 0.20539449155330658],\n",
              "  [4, 163, 0.5951874852180481, 0.5176249742507935, 0.2052987813949585],\n",
              "  [4, 164, 0.5951517820358276, 0.5173125267028809, 0.20532049238681793],\n",
              "  [4, 165, 0.595133900642395, 0.5173749923706055, 0.20526361465454102],\n",
              "  [4, 166, 0.5952767729759216, 0.5176874995231628, 0.2051917016506195],\n",
              "  [4, 167, 0.5951696634292603, 0.5170624852180481, 0.20512714982032776],\n",
              "  [4, 168, 0.5954107046127319, 0.5166875123977661, 0.20506972074508667],\n",
              "  [4, 169, 0.5956249833106995, 0.5168750286102295, 0.20500309765338898],\n",
              "  [4, 170, 0.5953035950660706, 0.5172500014305115, 0.20501326024532318],\n",
              "  [4, 171, 0.5957410931587219, 0.5166875123977661, 0.20496566593647003],\n",
              "  [4, 172, 0.5954464077949524, 0.5176874995231628, 0.20493201911449432],\n",
              "  [4, 173, 0.5957946181297302, 0.5166875123977661, 0.20491021871566772],\n",
              "  [4, 174, 0.595839262008667, 0.5166875123977661, 0.2048252522945404],\n",
              "  [4, 175, 0.5957410931587219, 0.5165625214576721, 0.20483466982841492],\n",
              "  [4, 176, 0.5962321162223816, 0.5164374709129333, 0.2048015147447586],\n",
              "  [4, 177, 0.5960714221000671, 0.5165625214576721, 0.2046733796596527],\n",
              "  [4, 178, 0.5957678556442261, 0.5166249871253967, 0.20462186634540558],\n",
              "  [4, 179, 0.5961339473724365, 0.5160624980926514, 0.20457908511161804],\n",
              "  [4, 180, 0.5962767601013184, 0.5162500143051147, 0.20451989769935608],\n",
              "  [4, 181, 0.5959464311599731, 0.5171250104904175, 0.20452724397182465],\n",
              "  [4, 182, 0.5961428284645081, 0.515999972820282, 0.20447184145450592],\n",
              "  [4, 183, 0.5963125228881836, 0.515874981880188, 0.2044399529695511],\n",
              "  [4, 184, 0.5965089201927185, 0.5156875252723694, 0.2043905258178711],\n",
              "  [4, 185, 0.5962142944335938, 0.515375018119812, 0.2043037861585617],\n",
              "  [4, 186, 0.5966339111328125, 0.5158125162124634, 0.2042902708053589],\n",
              "  [4, 187, 0.5968035459518433, 0.5154374837875366, 0.20422807335853577],\n",
              "  [4, 188, 0.596669614315033, 0.515874981880188, 0.20420175790786743],\n",
              "  [4, 189, 0.5965357422828674, 0.5151249766349792, 0.20415045320987701],\n",
              "  [4, 190, 0.5967321395874023, 0.5156875252723694, 0.20412370562553406],\n",
              "  [4, 191, 0.5969374775886536, 0.515375018119812, 0.20407307147979736],\n",
              "  [4, 192, 0.5970446467399597, 0.515500009059906, 0.2040574550628662],\n",
              "  [4, 193, 0.5963392853736877, 0.515500009059906, 0.2040829211473465],\n",
              "  [4, 194, 0.5969464182853699, 0.5162500143051147, 0.20411938428878784],\n",
              "  [4, 195, 0.5969107151031494, 0.5158125162124634, 0.20410872995853424],\n",
              "  [4, 196, 0.597428560256958, 0.5161250233650208, 0.2039152830839157],\n",
              "  [4, 197, 0.5968393087387085, 0.515874981880188, 0.20387084782123566],\n",
              "  [4, 198, 0.5968928337097168, 0.5159375071525574, 0.2039114534854889],\n",
              "  [4, 199, 0.5973392724990845, 0.5163124799728394, 0.20388464629650116],\n",
              "  [4, 200, 0.5971875190734863, 0.5167499780654907, 0.2037266492843628],\n",
              "  [5, 1, 0.5982053279876709, 0.5174375176429749, 0.22361159324645996],\n",
              "  [5, 2, 0.5988660454750061, 0.5166249871253967, 0.2180456966161728],\n",
              "  [5, 3, 0.598892867565155, 0.5168125033378601, 0.21680597960948944],\n",
              "  [5, 4, 0.5985803604125977, 0.5162500143051147, 0.21595026552677155],\n",
              "  [5, 5, 0.5981696248054504, 0.5171874761581421, 0.21557363867759705],\n",
              "  [5, 6, 0.5978214144706726, 0.5172500014305115, 0.21540047228336334],\n",
              "  [5, 7, 0.5974553823471069, 0.5180624723434448, 0.2153041660785675],\n",
              "  [5, 8, 0.597428560256958, 0.5181249976158142, 0.21526549756526947],\n",
              "  [5, 9, 0.5977767705917358, 0.5179374814033508, 0.21524862945079803],\n",
              "  [5, 10, 0.5975893139839172, 0.5180624723434448, 0.21520882844924927],\n",
              "  [5, 11, 0.5973125100135803, 0.5176874995231628, 0.2151370495557785],\n",
              "  [5, 12, 0.5969642996788025, 0.5170624852180481, 0.21504805982112885],\n",
              "  [5, 13, 0.5970625281333923, 0.5173749923706055, 0.21496005356311798],\n",
              "  [5, 14, 0.5974375009536743, 0.5167499780654907, 0.21487857401371002],\n",
              "  [5, 15, 0.5975178480148315, 0.5168750286102295, 0.21479327976703644],\n",
              "  [5, 16, 0.5975981950759888, 0.5172500014305115, 0.21472564339637756],\n",
              "  [5, 17, 0.5976428389549255, 0.5170000195503235, 0.21464811265468597],\n",
              "  [5, 18, 0.5977500081062317, 0.5173749923706055, 0.2145702987909317],\n",
              "  [5, 19, 0.5977857112884521, 0.5175625085830688, 0.2145020216703415],\n",
              "  [5, 20, 0.5980535745620728, 0.5178124904632568, 0.21441875398159027],\n",
              "  [5, 21, 0.5981249809265137, 0.5181249976158142, 0.21432429552078247],\n",
              "  [5, 22, 0.5982678532600403, 0.5181249976158142, 0.21422778069972992],\n",
              "  [5, 23, 0.5982053279876709, 0.5178124904632568, 0.21412144601345062],\n",
              "  [5, 24, 0.5982232093811035, 0.5174375176429749, 0.21402062475681305],\n",
              "  [5, 25, 0.5986339449882507, 0.5177500247955322, 0.21391575038433075],\n",
              "  [5, 26, 0.5990267992019653, 0.5181875228881836, 0.21380092203617096],\n",
              "  [5, 27, 0.599392831325531, 0.5183125138282776, 0.21369218826293945],\n",
              "  [5, 28, 0.5995357036590576, 0.5186874866485596, 0.21357524394989014],\n",
              "  [5, 29, 0.5997499823570251, 0.5190625190734863, 0.21345409750938416],\n",
              "  [5, 30, 0.5998035669326782, 0.5188124775886536, 0.2133329063653946],\n",
              "  [5, 31, 0.5997053384780884, 0.5191249847412109, 0.21321742236614227],\n",
              "  [5, 32, 0.5998928546905518, 0.5188124775886536, 0.21308563649654388],\n",
              "  [5, 33, 0.600151777267456, 0.5183749794960022, 0.2129453420639038],\n",
              "  [5, 34, 0.6002678275108337, 0.5184375047683716, 0.2128267139196396],\n",
              "  [5, 35, 0.6002589464187622, 0.5181875228881836, 0.21270814538002014],\n",
              "  [5, 36, 0.600428581237793, 0.5182499885559082, 0.21257561445236206],\n",
              "  [5, 37, 0.6005089282989502, 0.5184375047683716, 0.2124291956424713],\n",
              "  [5, 38, 0.6006249785423279, 0.5189999938011169, 0.21230949461460114],\n",
              "  [5, 39, 0.6005446314811707, 0.5181249976158142, 0.21218790113925934],\n",
              "  [5, 40, 0.6012410521507263, 0.5181875228881836, 0.21202626824378967],\n",
              "  [5, 41, 0.6017767786979675, 0.518625020980835, 0.2118617743253708],\n",
              "  [5, 42, 0.6012410521507263, 0.5183125138282776, 0.2117566019296646],\n",
              "  [5, 43, 0.6012321710586548, 0.518625020980835, 0.21158279478549957],\n",
              "  [5, 44, 0.6017410755157471, 0.5191249847412109, 0.21144728362560272],\n",
              "  [5, 45, 0.601937472820282, 0.518625020980835, 0.2112761288881302],\n",
              "  [5, 46, 0.601687490940094, 0.5192499756813049, 0.21115267276763916],\n",
              "  [5, 47, 0.6019999980926514, 0.5183749794960022, 0.21101771295070648],\n",
              "  [5, 48, 0.6027678847312927, 0.5192499756813049, 0.21087363362312317],\n",
              "  [5, 49, 0.6026785969734192, 0.5189999938011169, 0.21070505678653717],\n",
              "  [5, 50, 0.6029017567634583, 0.518875002861023, 0.21055032312870026],\n",
              "  [5, 51, 0.603026807308197, 0.5193750262260437, 0.21038518846035004],\n",
              "  [5, 52, 0.60284823179245, 0.5181875228881836, 0.21028238534927368],\n",
              "  [5, 53, 0.6028035879135132, 0.5184999704360962, 0.21010127663612366],\n",
              "  [5, 54, 0.6033750176429749, 0.5196874737739563, 0.2099333554506302],\n",
              "  [5, 55, 0.6033035516738892, 0.5186874866485596, 0.2097933292388916],\n",
              "  [5, 56, 0.6032232046127319, 0.5183749794960022, 0.20963716506958008],\n",
              "  [5, 57, 0.6037053465843201, 0.5189999938011169, 0.20950990915298462],\n",
              "  [5, 58, 0.6039464473724365, 0.5196874737739563, 0.2093133181333542],\n",
              "  [5, 59, 0.6037499904632568, 0.5189375281333923, 0.2091907113790512],\n",
              "  [5, 60, 0.6039196252822876, 0.518750011920929, 0.20908436179161072],\n",
              "  [5, 61, 0.6044999957084656, 0.5199375152587891, 0.20890562236309052],\n",
              "  [5, 62, 0.604357123374939, 0.518625020980835, 0.2087966948747635],\n",
              "  [5, 63, 0.6046339273452759, 0.5186874866485596, 0.20862552523612976],\n",
              "  [5, 64, 0.6049374938011169, 0.5192499756813049, 0.20842662453651428],\n",
              "  [5, 65, 0.6050267815589905, 0.5180624723434448, 0.2083165943622589],\n",
              "  [5, 66, 0.6047946214675903, 0.5184999704360962, 0.20818693935871124],\n",
              "  [5, 67, 0.6050714254379272, 0.518875002861023, 0.20805329084396362],\n",
              "  [5, 68, 0.6057232022285461, 0.518625020980835, 0.20789487659931183],\n",
              "  [5, 69, 0.6056874990463257, 0.5184375047683716, 0.20774084329605103],\n",
              "  [5, 70, 0.6056607365608215, 0.5180000066757202, 0.2076629251241684],\n",
              "  [5, 71, 0.6057500243186951, 0.518750011920929, 0.2075076401233673],\n",
              "  [5, 72, 0.6056874990463257, 0.5181249976158142, 0.2074073851108551],\n",
              "  [5, 73, 0.6059107184410095, 0.5179374814033508, 0.20729802548885345],\n",
              "  [5, 74, 0.6062856912612915, 0.518625020980835, 0.20713259279727936],\n",
              "  [5, 75, 0.6060535907745361, 0.5181875228881836, 0.2069987654685974],\n",
              "  [5, 76, 0.6061607003211975, 0.5176874995231628, 0.20684322714805603],\n",
              "  [5, 77, 0.606580376625061, 0.5183125138282776, 0.2067452371120453],\n",
              "  [5, 78, 0.6063392758369446, 0.5177500247955322, 0.2066386193037033],\n",
              "  [5, 79, 0.6059732437133789, 0.5181249976158142, 0.2064695507287979],\n",
              "  [5, 80, 0.6068928837776184, 0.5178750157356262, 0.20644056797027588],\n",
              "  [5, 81, 0.6065714359283447, 0.5181249976158142, 0.20635859668254852],\n",
              "  [5, 82, 0.6067589521408081, 0.5173125267028809, 0.20623689889907837],\n",
              "  [5, 83, 0.6069821715354919, 0.5172500014305115, 0.2060275375843048],\n",
              "  [5, 84, 0.6071696281433105, 0.5178750157356262, 0.2058701366186142],\n",
              "  [5, 85, 0.6077053546905518, 0.5171250104904175, 0.205876886844635],\n",
              "  [5, 86, 0.6071875095367432, 0.5173749923706055, 0.2058805227279663],\n",
              "  [5, 87, 0.607205331325531, 0.5174375176429749, 0.20558391511440277],\n",
              "  [5, 88, 0.6077678799629211, 0.5174999833106995, 0.2054864466190338],\n",
              "  [5, 89, 0.6076428294181824, 0.5170624852180481, 0.20551875233650208],\n",
              "  [5, 90, 0.6079464554786682, 0.5165625214576721, 0.20533259212970734],\n",
              "  [5, 91, 0.6082053780555725, 0.5173749923706055, 0.20519764721393585],\n",
              "  [5, 92, 0.6076964139938354, 0.5165625214576721, 0.20518220961093903],\n",
              "  [5, 93, 0.60808926820755, 0.5168750286102295, 0.20498710870742798],\n",
              "  [5, 94, 0.6084374785423279, 0.5173125267028809, 0.20489008724689484],\n",
              "  [5, 95, 0.6085000038146973, 0.5173749923706055, 0.20480704307556152],\n",
              "  [5, 96, 0.608919620513916, 0.5165625214576721, 0.20468486845493317],\n",
              "  [5, 97, 0.6087678670883179, 0.5168125033378601, 0.20453891158103943],\n",
              "  [5, 98, 0.6091428399085999, 0.5164999961853027, 0.2044644057750702],\n",
              "  [5, 99, 0.6097946166992188, 0.5174375176429749, 0.20433621108531952],\n",
              "  [5, 100, 0.6092321276664734, 0.5174375176429749, 0.20424838364124298],\n",
              "  [5, 101, 0.6095268130302429, 0.5168125033378601, 0.20414526760578156],\n",
              "  [5, 102, 0.6101696491241455, 0.5170000195503235, 0.20401321351528168],\n",
              "  [5, 103, 0.6100446581840515, 0.5174375176429749, 0.20399382710456848],\n",
              "  [5, 104, 0.6100446581840515, 0.5173125267028809, 0.2038518190383911],\n",
              "  [5, 105, 0.6102321147918701, 0.5165625214576721, 0.2037232518196106],\n",
              "  [5, 106, 0.6105714440345764, 0.5178124904632568, 0.20368042588233948],\n",
              "  [5, 107, 0.6104910969734192, 0.5171874761581421, 0.20356708765029907],\n",
              "  [5, 108, 0.6106071472167969, 0.5171250104904175, 0.20349079370498657],\n",
              "  [5, 109, 0.6110357046127319, 0.5171874761581421, 0.20346683263778687],\n",
              "  [5, 110, 0.6112589240074158, 0.5171250104904175, 0.20330339670181274],\n",
              "  [5, 111, 0.6104910969734192, 0.5170000195503235, 0.20328651368618011],\n",
              "  [5, 112, 0.6111785769462585, 0.5169374942779541, 0.20324264466762543],\n",
              "  [5, 113, 0.6111160516738892, 0.5170624852180481, 0.20320731401443481],\n",
              "  [5, 114, 0.6114910840988159, 0.5170000195503235, 0.20311954617500305],\n",
              "  [5, 115, 0.6116428375244141, 0.5166875123977661, 0.2029457837343216],\n",
              "  [5, 116, 0.6117053627967834, 0.5171874761581421, 0.2029276192188263],\n",
              "  [5, 117, 0.6117143034934998, 0.5171250104904175, 0.2028043270111084],\n",
              "  [5, 118, 0.6117410659790039, 0.5163750052452087, 0.20275308191776276],\n",
              "  [5, 119, 0.6118035912513733, 0.5171250104904175, 0.20270854234695435],\n",
              "  [5, 120, 0.6117767691612244, 0.5168125033378601, 0.2026219666004181],\n",
              "  [5, 121, 0.6123303771018982, 0.5165625214576721, 0.2025032490491867],\n",
              "  [5, 122, 0.6121518015861511, 0.5171250104904175, 0.20243896543979645],\n",
              "  [5, 123, 0.6122857332229614, 0.5174999833106995, 0.20243842899799347],\n",
              "  [5, 124, 0.6122410893440247, 0.5173125267028809, 0.20233428478240967],\n",
              "  [5, 125, 0.6126339435577393, 0.5173749923706055, 0.20226682722568512],\n",
              "  [5, 126, 0.612625002861023, 0.5177500247955322, 0.20217956602573395],\n",
              "  [5, 127, 0.6127767562866211, 0.5172500014305115, 0.20216771960258484],\n",
              "  [5, 128, 0.6120803356170654, 0.5170624852180481, 0.20213766396045685],\n",
              "  [5, 129, 0.6127499938011169, 0.5170624852180481, 0.20208126306533813],\n",
              "  [5, 130, 0.6125803589820862, 0.5166249871253967, 0.2019752711057663],\n",
              "  [5, 131, 0.6127410531044006, 0.5171874761581421, 0.20188793540000916],\n",
              "  [5, 132, 0.6128303408622742, 0.5170624852180481, 0.20184747874736786],\n",
              "  [5, 133, 0.6126964092254639, 0.5174375176429749, 0.20175205171108246],\n",
              "  [5, 134, 0.6130803823471069, 0.5178750157356262, 0.20171210169792175],\n",
              "  [5, 135, 0.613053560256958, 0.5174999833106995, 0.20173828303813934],\n",
              "  [5, 136, 0.6125803589820862, 0.5178124904632568, 0.20166908204555511],\n",
              "  [5, 137, 0.6130178570747375, 0.5180000066757202, 0.2015245258808136],\n",
              "  [5, 138, 0.6135267615318298, 0.5180000066757202, 0.20147739350795746],\n",
              "  [5, 139, 0.6131964325904846, 0.5178750157356262, 0.20147040486335754],\n",
              "  [5, 140, 0.6130892634391785, 0.5178750157356262, 0.20143188536167145],\n",
              "  [5, 141, 0.6134553551673889, 0.5179374814033508, 0.20145165920257568],\n",
              "  [5, 142, 0.6133481860160828, 0.5180624723434448, 0.2012776881456375],\n",
              "  [5, 143, 0.6133481860160828, 0.5177500247955322, 0.20129530131816864],\n",
              "  [5, 144, 0.6138035655021667, 0.5182499885559082, 0.20130398869514465],\n",
              "  [5, 145, 0.6136428713798523, 0.5181249976158142, 0.2012471854686737],\n",
              "  [5, 146, 0.6134374737739563, 0.5174375176429749, 0.20110322535037994],\n",
              "  [5, 147, 0.6139017939567566, 0.5176874995231628, 0.20106323063373566],\n",
              "  [5, 148, 0.6140535473823547, 0.5181249976158142, 0.20101597905158997],\n",
              "  [5, 149, 0.6136071681976318, 0.5183125138282776, 0.20106500387191772],\n",
              "  [5, 150, 0.6134999990463257, 0.5173125267028809, 0.20108357071876526],\n",
              "  [5, 151, 0.6141071319580078, 0.518625020980835, 0.2009960561990738],\n",
              "  [5, 152, 0.6138392686843872, 0.5180624723434448, 0.20087671279907227],\n",
              "  [5, 153, 0.6134374737739563, 0.5179374814033508, 0.2008461207151413],\n",
              "  [5, 154, 0.6143303513526917, 0.5175625085830688, 0.20088696479797363],\n",
              "  [5, 155, 0.6141339540481567, 0.5175625085830688, 0.2008310705423355],\n",
              "  [5, 156, 0.6138482093811035, 0.5178750157356262, 0.2006470263004303],\n",
              "  [5, 157, 0.6141785979270935, 0.5181875228881836, 0.20063123106956482],\n",
              "  [5, 158, 0.6144285798072815, 0.5182499885559082, 0.2005954086780548],\n",
              "  [5, 159, 0.6141250133514404, 0.5173125267028809, 0.2006041407585144],\n",
              "  [5, 160, 0.614517867565155, 0.5178750157356262, 0.20047695934772491],\n",
              "  [5, 161, 0.6142321228981018, 0.5175625085830688, 0.20041418075561523],\n",
              "  [5, 162, 0.6142500042915344, 0.5171250104904175, 0.20036360621452332],\n",
              "  [5, 163, 0.6146785616874695, 0.5183749794960022, 0.20037731528282166],\n",
              "  [5, 164, 0.6146696209907532, 0.5176249742507935, 0.2003115564584732],\n",
              "  [5, 165, 0.6147589087486267, 0.5180624723434448, 0.200184166431427],\n",
              "  [5, 166, 0.6146607398986816, 0.5181875228881836, 0.20015965402126312],\n",
              "  [5, 167, 0.614464282989502, 0.5183749794960022, 0.20019711554050446],\n",
              "  [5, 168, 0.6147857308387756, 0.5177500247955322, 0.2001553773880005],\n",
              "  [5, 169, 0.6150624752044678, 0.5180624723434448, 0.19998995959758759],\n",
              "  [5, 170, 0.6146517992019653, 0.5176874995231628, 0.2000519037246704],\n",
              "  [5, 171, 0.614642858505249, 0.5177500247955322, 0.19998890161514282],\n",
              "  [5, 172, 0.6151160597801208, 0.5186874866485596, 0.19993019104003906],\n",
              "  [5, 173, 0.614589273929596, 0.518625020980835, 0.19991534948349],\n",
              "  [5, 174, 0.6146785616874695, 0.5185624957084656, 0.19984877109527588],\n",
              "  [5, 175, 0.6149821281433105, 0.5186874866485596, 0.19976745545864105],\n",
              "  [5, 176, 0.6148303747177124, 0.5184999704360962, 0.19971083104610443],\n",
              "  [5, 177, 0.6147410869598389, 0.518875002861023, 0.19976992905139923],\n",
              "  [5, 178, 0.615017831325531, 0.5183125138282776, 0.19975614547729492],\n",
              "  [5, 179, 0.6150446534156799, 0.5189375281333923, 0.1996217519044876],\n",
              "  [5, 180, 0.6149464249610901, 0.5191249847412109, 0.1996140033006668],\n",
              "  [5, 181, 0.6147767901420593, 0.5184375047683716, 0.19973470270633698],\n",
              "  [5, 182, 0.6153125166893005, 0.518625020980835, 0.1996767222881317],\n",
              "  [5, 183, 0.6152678728103638, 0.5191249847412109, 0.1995794177055359],\n",
              "  [5, 184, 0.6148214340209961, 0.5190625190734863, 0.19955632090568542],\n",
              "  [5, 185, 0.6153125166893005, 0.5189375281333923, 0.19949717819690704],\n",
              "  [5, 186, 0.615776777267456, 0.518625020980835, 0.19946905970573425],\n",
              "  [5, 187, 0.6151964068412781, 0.518750011920929, 0.1994050145149231],\n",
              "  [5, 188, 0.6151607036590576, 0.5193750262260437, 0.19942913949489594],\n",
              "  [5, 189, 0.6157857179641724, 0.5188124775886536, 0.19932550191879272],\n",
              "  [5, 190, 0.6153392791748047, 0.5195000171661377, 0.19934776425361633],\n",
              "  [5, 191, 0.6155267953872681, 0.5192499756813049, 0.19928303360939026],\n",
              "  [5, 192, 0.6154553294181824, 0.5186874866485596, 0.1992390900850296],\n",
              "  [5, 193, 0.6154821515083313, 0.5191249847412109, 0.19920235872268677],\n",
              "  [5, 194, 0.615473210811615, 0.5193750262260437, 0.1991594433784485],\n",
              "  [5, 195, 0.6152946352958679, 0.5184999704360962, 0.19905199110507965],\n",
              "  [5, 196, 0.6155178546905518, 0.5188124775886536, 0.19909824430942535],\n",
              "  [5, 197, 0.6157321333885193, 0.518875002861023, 0.19914866983890533],\n",
              "  [5, 198, 0.615723192691803, 0.5191249847412109, 0.19898034632205963],\n",
              "  [5, 199, 0.6154642701148987, 0.5192499756813049, 0.19902974367141724],\n",
              "  [5, 200, 0.6158660650253296, 0.5180624723434448, 0.19899484515190125],\n",
              "  ...])"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ],
      "source": [
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
        "\n",
        "dataset = TensorDataset(X_tensor, Y_tensor)\n",
        "train_set, test_set, val_set = split_dataset(dataset)\n",
        "\n",
        "train_data = next(iter(DataLoader(train_set, batch_size=len(train_set))))\n",
        "test_data = next(iter(DataLoader(test_set, batch_size=len(test_set))))\n",
        "val_data = next(iter(DataLoader(val_set, batch_size=len(val_set))))\n",
        "\n",
        "# Unpack data\n",
        "X_train, y_train = train_data\n",
        "X_val, y_val = val_data\n",
        "X_test, y_test = test_data\n",
        "\n",
        "# --- Train ---\n",
        "model = GBNN(n_stages=35)\n",
        "train_gbnn(model, X_train, y_train, X_val, y_val, X_test, y_test, n_epochs=200, lr=0.01)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UCaHbnSZvFga"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "iIfhN7_-DBO0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Predict 32-bit output from 32-bit input\n",
        "def predict_single_32bit(model: nn.Module, input_bits: list, threshold: float = 0.5) -> list:\n",
        "    assert len(input_bits) == 32, \"Input must be a 32-bit binary list.\"\n",
        "\n",
        "    model.eval()\n",
        "    input_array = np.array(input_bits, dtype=np.float32)\n",
        "    normalized_input = input_array * 0.98 + 0.01\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_tensor = torch.tensor(normalized_input).unsqueeze(0)  # Shape: [1, 32]\n",
        "        logits = model(input_tensor)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        prediction = (probs > threshold).int().squeeze().tolist()\n",
        "\n",
        "    return prediction\n",
        "\n",
        "def predict_batch_32bit(model: nn.Module, input_array: np.ndarray, threshold: float = 0.5) -> list:\n",
        "    assert input_array.ndim == 2 and input_array.shape[1] == 32, \"Input must be of shape [N, 32]\"\n",
        "\n",
        "    model.eval()\n",
        "    normalized_input = input_array * 0.98 + 0.01\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_tensor = torch.tensor(normalized_input, dtype=torch.float32)\n",
        "        logits = model(input_tensor)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        predictions = (probs > threshold).int().tolist()  # List of [0,1] lists\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def evaluate_predictions(y_true: np.ndarray, y_pred: np.ndarray):\n",
        "\n",
        "    assert y_true.shape == y_pred.shape, \"Shape mismatch between true and predicted labels\"\n",
        "\n",
        "    metrics = {}\n",
        "\n",
        "    # Bitwise metrics (macro = average over each bit)\n",
        "    metrics[\"bitwise_accuracy\"] = accuracy_score(y_true.flatten(), y_pred.flatten())\n",
        "    metrics[\"macro_precision\"] = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    metrics[\"macro_recall\"] = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    metrics[\"macro_f1\"] = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "    # Hamming loss (fraction of incorrect bits)\n",
        "    metrics[\"hamming_loss\"] = hamming_loss(y_true, y_pred)\n",
        "\n",
        "    # Exact match (all 32 bits correct)\n",
        "    exact_matches = np.all(y_true == y_pred, axis=1)\n",
        "    metrics[\"exact_match_ratio\"] = np.mean(exact_matches)\n",
        "\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "N0lZ3vyDs_uW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9634063c-2608-4a1b-b82d-98b6f3db69c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicted Output Bits: [1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Predict a 32-bit binary input\n",
        "input_bits = [0, 1, 0, 1, 0, 1, 1, 0,\n",
        "              1, 0, 0, 1, 1, 0, 0, 1,\n",
        "              1, 1, 0, 0, 1, 0, 1, 0,\n",
        "              1, 1, 0, 0, 1, 0, 0, 1]\n",
        "\n",
        "predicted_output = predict_single_32bit(model, input_bits)\n",
        "print(\"\\nPredicted Output Bits:\", predicted_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "7DASoLyVs_xv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59c3fc35-523b-47fb-8f5d-5084c69e0429"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions shape: (5000, 32)\n",
            "First prediction: [1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "X = challenge_df.values.astype(np.float32)  # shape [N, 32]\n",
        "\n",
        "# Predict all rows\n",
        "predictions = predict_batch_32bit(model, X)\n",
        "\n",
        "\n",
        "predictions_np = np.array(predictions)\n",
        "\n",
        "print(\"Predictions shape:\", predictions_np.shape)\n",
        "print(\"First prediction:\", predictions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "mTL713UEga34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "d10bc84b-27d8-4b5c-d4f9-4d2306c8a271"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'accuracy_score' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-132-89d7583e72b5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-129-8b227b4786a4>\u001b[0m in \u001b[0;36mevaluate_predictions\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Bitwise metrics (macro = average over each bit)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bitwise_accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"macro_precision\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"macro_recall\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "Y_true = Y.astype(np.int32)\n",
        "X_input = X.astype(np.float32)\n",
        "Y_pred = np.array(predict_batch_32bit(model, X_input))\n",
        "\n",
        "# Evaluate\n",
        "metrics = evaluate_predictions(Y_true, Y_pred)\n",
        "\n",
        "\n",
        "for name, value in metrics.items():\n",
        "    print(f\"{name}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnBnJi6gga6m"
      },
      "outputs": [],
      "source": [
        "\n",
        "def top_k_bit_errors(y_true, y_pred, k=5):\n",
        "\n",
        "\n",
        "    bit_errors_per_sample = np.sum(y_true != y_pred, axis=1)\n",
        "\n",
        "\n",
        "    top_k_indices = np.argsort(bit_errors_per_sample)[-k:][::-1]\n",
        "\n",
        "    top_k_errors = bit_errors_per_sample[top_k_indices]\n",
        "\n",
        "    return top_k_indices, top_k_errors\n",
        "\n",
        "\n",
        "top_indices, top_errors = top_k_bit_errors(Y_true, Y_pred, k=5)\n",
        "\n",
        "print(\"Top 5 samples with highest bit errors:\")\n",
        "for rank, (idx, errors) in enumerate(zip(top_indices, top_errors), 1):\n",
        "    print(f\"Rank {rank}: Sample index {idx}, Bit errors = {errors}\")\n",
        "    print(f\"True bits: {Y_true[idx]}\")\n",
        "    print(f\"Pred bits: {Y_pred[idx]}\")\n",
        "    print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ivgrcxcyga-M"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1EShK-jDzkI3leiaPV8slaioA23_mmTiC",
      "authorship_tag": "ABX9TyOfYX0Y6tMOu1svfU27yM0N",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}